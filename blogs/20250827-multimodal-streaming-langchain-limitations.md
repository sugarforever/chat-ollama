# 20250827 - üñºÔ∏èüì° Multimodal Content Streaming: Bridging LangChain OpenAI Package Limitations

*August 27, 2025*

## The Challenge

When building ChatOllama's support for multimodal AI interactions, we encountered a significant limitation in the `@langchain/openai` package. While OpenAI-compatible APIs (like OpenRouter with Gemini models) were successfully generating and returning images alongside text content, these images were completely lost in the streaming pipeline. Users would see text responses but miss the generated images entirely.

This wasn't just a minor feature gap - it was a fundamental architectural limitation that prevented our platform from supporting the rich, multimodal AI experiences that modern language models are capable of providing.

## What We Discovered

During our investigation, we identified several critical issues in how the LangChain OpenAI package handles multimodal content:

### 1. **The Missing Images Field**

OpenAI-compatible APIs return multimodal content in this format:

```json
{
  "role": "assistant",
  "content": "Here is an image for you: ",
  "images": [
    {
      "type": "image_url",
      "image_url": {
        "url": "data:image/png;base64,iVBORw0KGgo...",
        "detail": "high"
      }
    }
  ]
}
```

However, LangChain's message conversion functions completely ignore the `images` field, only processing `content`, `tool_calls`, `function_call`, and `audio` fields.

**Original LangChain Implementation (Before Our Fix):**

```typescript
// _convertCompletionsDeltaToBaseMessageChunk - Original LangChain code
protected _convertCompletionsDeltaToBaseMessageChunk(
  delta: Record<string, any>,
  rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk,
  defaultRole?: OpenAIClient.Chat.ChatCompletionRole
) {
  const role = delta.role ?? defaultRole
  let content = delta.content ?? ""  // Only processes content field

  let additional_kwargs: Record<string, unknown>
  if (delta.function_call) {                    // Processes function_call
    additional_kwargs = { function_call: delta.function_call }
  } else if (delta.tool_calls) {                // Processes tool_calls
    additional_kwargs = { tool_calls: delta.tool_calls }
  } else {
    additional_kwargs = {}
  }

  if (delta.audio) {                            // Processes audio field
    additional_kwargs.audio = {
      ...delta.audio,
      index: rawResponse.choices[0].index,
    }
  }
  
  // NOTE: No handling for delta.images field!
  // Images are completely ignored and lost

  return new AIMessageChunk({ content, ... })
}
```

As you can see, there's no logic to handle `delta.images` - the field is completely ignored, causing all image content to be lost.

### 2. **Streaming Delta Conversion Gaps**

The `_convertCompletionsDeltaToBaseMessageChunk()` function in LangChain processes streaming deltas but has no logic to handle image content. This means that even when APIs stream images, they're silently dropped.

**Similar Issue in Non-Streaming Conversion:**

```typescript
// _convertCompletionsMessageToBaseMessage - Original LangChain code
protected _convertCompletionsMessageToBaseMessage(
  message: OpenAIClient.Chat.Completions.ChatCompletionMessage,
  rawResponse: OpenAIClient.Chat.Completions.ChatCompletion
): BaseMessage {
  // Process tool_calls, function_call, audio...
  const additional_kwargs: Record<string, unknown> = {
    function_call: message.function_call,     // Processes function_call
    tool_calls: rawToolCalls,                 // Processes tool_calls
  }
  
  if (message.audio) {                        // Processes audio
    additional_kwargs.audio = message.audio
  }

  // Only processes the content field as string
  const content = message.content || ""       // No images handling!

  return new AIMessage({
    content,  // Images field completely ignored
    tool_calls: toolCalls,
    additional_kwargs,
    // ...
  })
}
```

The same pattern exists in non-streaming message conversion - `message.images` is never processed.

### 3. **Token Counting Complications**

LangChain's token counting and callback systems expect string content, but multimodal content requires array formats. This created a conflict where supporting images would break existing token tracking mechanisms.

### 4. **Chat API Streaming Limitations**

Our chat API's content extraction was also text-only, compounding the problem by not preserving image data even if it made it through the LangChain layer.

## LangChain's Expected Format vs Reality

LangChain expects multimodal content in this array format:

```json
{
  "role": "assistant", 
  "content": [
    { "type": "text", "text": "Here is an image for you: " },
    { 
      "type": "image_url", 
      "image_url": { 
        "url": "data:image/png;base64,iVBORw0KGgo...",
        "detail": "high"
      }
    }
  ]
}
```

The fundamental mismatch between what APIs provide (`images` field) and what LangChain processes (array `content`) created the integration gap.

## Our Solution: Bridging the Gap

Rather than waiting for LangChain to add official multimodal support, we implemented a comprehensive solution that transforms API responses into LangChain-compatible formats while preserving all functionality.

### 1. Enhanced Message Conversion Functions

We modified LangChain's core message conversion functions to process the `images` field:

#### Streaming Delta Conversion

```typescript
// Before: Only handled content as string
let content = delta.content ?? ""

// After: Process images field and convert to array format
let content = delta.content ?? ""

// Handle images field that might contain image_url content
if (delta.images && Array.isArray(delta.images)) {
  if (typeof content === "string") {
    const contentArray = []
    if (content) {
      contentArray.push({ type: "text", text: content })
    }
    // Add image content from the images field
    for (const image of delta.images) {
      if (image.type === "image_url" && image.image_url) {
        contentArray.push({
          type: "image_url",
          image_url: image.image_url,
        })
      }
    }
    content = contentArray
  }
}
```

The same logic was applied to non-streaming message conversion, ensuring consistent behavior across all response types.

### 2. Enhanced Chat API Streaming

#### Problem: Text-Only Content Extraction

Our chat API's `extractContentFromChunk()` function only extracted text:

```typescript
// Before: Only text extraction (Original Implementation)
const extractContentFromChunk = (chunk: BaseMessageChunk): string => {
  let content = chunk?.content
  // Handle array of text_delta objects
  if (Array.isArray(content)) {
    content = content
      .filter(item => item.type === 'text_delta' || item.type === 'text')
      .map(item => ('text' in item ? item.text : ''))
      .join('')
  }
  return content || ''
  // NOTE: Images are completely ignored!
  // Even if LangChain passed them through, they'd be lost here
}
```

This function only returned a string, making it impossible to preserve image data even if it made it through the LangChain layer.

#### Solution: Multimodal Content Extraction

```typescript
// After: Extract both text and images
const extractContentFromChunk = (chunk: BaseMessageChunk): { text: string; images: any[] } => {
  let content = chunk?.content
  let textContent = ''
  let images: any[] = []
  
  if (Array.isArray(content)) {
    // Extract text content
    textContent = content
      .filter(item => item.type === 'text_delta' || item.type === 'text')
      .map(item => ('text' in item ? item.text : ''))
      .join('')
    
    // Extract image content
    images = content
      .filter(item => item.type === 'image_url' && item.image_url?.url)
      .map(item => ({ type: 'image_url', image_url: item.image_url }))
  } else {
    textContent = content || ''
  }
  
  return { text: textContent, images }
}
```

### 3. Intelligent Content Accumulation

#### Problem: Text-Only Streaming Accumulation

The original streaming logic only handled text content:

```typescript
// Before: Original streaming accumulation (text-only)
let accumulatedContent = ''

for await (const chunk of stream) {
  const { text } = extractContentFromChunk(chunk)  // Only extracted text
  accumulatedContent += text  // Only accumulated text
  
  // Stream only text content to client
  await streamToClient({
    id: 'msg-123',
    content: accumulatedContent,  // String only - no images!
    // Images were lost at this point even if they existed
  })
}

// Final message also text-only
const finalMessage = {
  role: 'assistant',
  content: accumulatedContent  // String format - multimodal data lost
}
```

This approach meant that even if LangChain somehow passed through image data, it would be discarded during accumulation.

#### Solution: Dynamic Multimodal Accumulation

The enhanced streaming logic handles both text and images dynamically:

```typescript
// After: Enhanced streaming accumulation (multimodal)
let accumulatedTextContent = ''
let accumulatedImages: any[] = []

for await (const chunk of stream) {
  const { text, images } = extractContentFromChunk(chunk)  // Extract both
  
  // Accumulate text content
  accumulatedTextContent += text
  
  // Accumulate unique images (avoid duplicates)
  for (const image of images) {
    if (!accumulatedImages.some(img => img.image_url?.url === image.image_url?.url)) {
      accumulatedImages.push(image)
    }
  }
  
  // Dynamic content format conversion
  let contentToStream: string | MessageContent[]
  if (accumulatedImages.length > 0) {
    const contentArray: MessageContent[] = []
    if (accumulatedTextContent) {
      contentArray.push({ type: 'text', text: accumulatedTextContent })
    }
    contentArray.push(...accumulatedImages)
    contentToStream = contentArray
  } else {
    contentToStream = accumulatedTextContent
  }
  
  // Stream multimodal content to client
  await streamToClient({
    id: 'msg-123',
    content: contentToStream,  // String OR array - preserves all data!
  })
}

// Final message preserves multimodal structure
const finalMessage = {
  role: 'assistant',
  content: contentToStream  // Maintains original format
}
```

This approach ensures that:
- Text-only responses remain as strings (maintaining performance)
- Multimodal responses automatically convert to array format
- Content is accumulated progressively during streaming

### 4. Frontend Already Equipped

#### Fortunate Architecture: Frontend Was Ready

Interestingly, our Vue.js frontend was already designed to handle multimodal content properly. This shows good architectural foresight:

```vue
<!-- Original ChatMessageItem.vue template (already multimodal-ready) -->
<template>
  <div class="message-container">
    <!-- Text content with markdown rendering -->
    <div class="prose" v-html="markdown.render(messageContent)" />
    
    <!-- Image gallery (already present but never used due to backend limitations) -->
    <div v-if="messageImages.length > 0" class="image-gallery">
      <img v-for="(url, index) in messageImages"
           :key="index"
           :src="url"
           class="message-image"
           :alt="`Image ${index + 1}`" />
    </div>
  </div>
</template>
```

```typescript
// Original computed properties (already handling both formats)
const messageImages = computed(() => {
  const content = props.message.content
  if (!content || !Array.isArray(content)) return []

  return content
    .filter(item => item.type === 'image_url' && item.image_url?.url)
    .map(item => item.image_url!.url)
})

const messageContent = computed(() => {
  const content = props.message.content
  if (!content) return ''

  if (Array.isArray(content)) {
    return content
      .filter(item => item.type === 'text')
      .map(item => item.text)
      .filter(Boolean)
      .join('\n')
  }
  return content  // Handle string content (backward compatibility)
})
```

The frontend was already prepared for multimodal content - it just never received any due to the backend limitations!

#### What We Improved: Styling and Semantic Order

We only needed to make minor improvements:

```vue
<!-- After: Improved styling with Tailwind and semantic ordering -->
<template>
  <div class="message-container">
    <!-- Text content rendered first (semantic priority) -->
    <div v-html="markdown.render(messageContent)" />
    
    <!-- Images below text content with responsive grid -->
    <div v-if="messageImages.length > 0" 
         class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-2 mt-3">
      <img v-for="(url, index) in messageImages"
           :src="url"
           class="w-full h-auto rounded-lg max-h-64 object-contain bg-gray-100 dark:bg-gray-800"
           :alt="`Image ${index + 1}`" />
    </div>
  </div>
</template>
```

Changes made:
- Replaced custom CSS with Tailwind utility classes
- Added responsive grid layout (`grid-cols-1 sm:grid-cols-2 lg:grid-cols-3`)
- Ensured images appear below text content semantically
- Added proper spacing with `mt-3`

## Key Technical Decisions

### 1. Backward Compatibility First

We ensured that existing text-only functionality remained unchanged:
- String content stays as strings
- Only convert to arrays when images are present
- No breaking changes for current users

### 2. Token Handling Strategy

LangChain's callbacks expect text for token counting, so we:
- Extract text from multimodal content for callbacks
- Preserve the full multimodal structure for rendering
- Maintain separate pipelines for tracking vs. display

### 3. Semantic Content Ordering

Following natural generation patterns:
- Text content appears first
- Images follow below text
- Reflects how AI models typically generate content

### 4. Clean Production Code

- Removed all debug logging from production builds
- Replaced custom CSS with Tailwind utility classes
- Optimized for performance and maintainability

## Performance Considerations

### Streaming Efficiency
- Content updates stream immediately to prevent timeouts
- Progressive accumulation of both text and images
- Efficient memory management without leaks

### Content Processing
- Minimal overhead when no images are present
- Optimized validation and filtering operations
- Dynamic format conversion only when needed

## Testing and Validation

We implemented comprehensive testing covering:

### Core Logic Tests
- Single image with text scenarios
- Multiple images with text
- Images-only responses (empty text)
- Text-only responses (backward compatibility)
- Invalid image object handling
- Edge cases and error conditions

### Integration Testing
- End-to-end multimodal streaming
- Frontend rendering validation
- Performance benchmarking
- Cross-browser compatibility

## Real-World Usage

Here's how the system handles a typical multimodal response:

### Input (OpenAI-compatible API)
```json
{
  "role": "assistant",
  "content": "Here are two data visualizations: ",
  "images": [
    { "type": "image_url", "image_url": { "url": "data:image/png;base64,chart1..." } },
    { "type": "image_url", "image_url": { "url": "data:image/png;base64,chart2..." } }
  ]
}
```

### Output (LangChain Compatible)
```json
{
  "role": "assistant",
  "content": [
    { "type": "text", "text": "Here are two data visualizations: " },
    { "type": "image_url", "image_url": { "url": "data:image/png;base64,chart1..." } },
    { "type": "image_url", "image_url": { "url": "data:image/png;base64,chart2..." } }
  ]
}
```

### Frontend Result
Users see the explanatory text followed by both chart images in a responsive grid layout, creating a seamless multimodal experience.

## Future Considerations

### LangChain Evolution
- Monitor official multimodal support development
- Prepare migration path when native support arrives
- Maintain version compatibility during transition

### Extended Media Support
The architecture easily extends to support:
- Video content streaming
- Audio file attachments
- Document previews
- Interactive media elements

### Provider Expansion
Works with any OpenAI-compatible API that uses the `images` field pattern:
- OpenRouter with various models
- Custom inference endpoints
- Future multimodal API providers

## Lessons Learned

This implementation taught us several important principles:

### Technical Debt Management
- Early pragmatic decisions (text-only) enabled rapid development
- Clear criteria for when to address limitations prevents user impact
- Incremental enhancement preserves existing functionality

### Integration Patterns
- Always verify actual API schemas vs. documentation
- Build graceful fallbacks for external service failures
- Design for backward compatibility from the start

### Performance Optimization
- Parallel processing over sequential API calls
- Dynamic format conversion based on content type
- Efficient validation with minimal overhead

## Conclusion

This multimodal streaming implementation successfully bridges a critical gap in the LangChain ecosystem while maintaining full backward compatibility. By transforming OpenAI-compatible API responses into LangChain's expected format, we've enabled rich multimodal experiences in ChatOllama without waiting for upstream library updates.

The solution demonstrates how thoughtful architecture can work around third-party limitations while building toward a future where such workarounds become unnecessary. As the AI ecosystem evolves and LangChain adds native multimodal support, our implementation provides a clear migration path.

Most importantly, this enhancement unlocks the full potential of modern multimodal AI models for ChatOllama users, enabling them to receive and interact with the rich, visual content that represents the cutting edge of AI capabilities.

---

**Files Modified**: 
- `/server/models/openai/chat_models.ts`
- `/server/api/models/chat/index.post.ts`
- `/components/ChatMessageItem.vue`

**Dependencies**: LangChain ^0.1.31, OpenAI SDK ^4.33.0, Vue 3, Nuxt 3

**Status**: Production Ready ‚úÖ