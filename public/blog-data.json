{
  "posts": [
    {
      "slug": "20250911-building-contextual-quick-chat-inspired-by-ai-ides",
      "title": "Building Contextual Quick Chat: When AI IDEs Inspire Web Applications",
      "date": "2025-09-11",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# Building Contextual Quick Chat: When AI IDEs Inspire Web Applications\n\n> How we brought the familiar \"quick edit\" experience from AI IDEs to web-based chat applications, solving text selection persistence and creating a truly contextual AI assistant.\n\n## The Inspiration: AI IDEs Done Right\n\nIf you've used modern AI-powered IDEs like Cursor, GitHub Copilot, or Claude Code, you've experienced a delightful interaction pattern: select some code, right-click or use a keyboard shortcut, and instantly get contextual AI assistance in a compact dialog. No context switching, no copy-pasting, no losing your place in the code.\n\nThis interaction is so intuitive that when users encounter it, they immediately understand what to do. The selected text provides perfect context, the dialog appears exactly where needed, and the AI assistance feels truly integrated into the workflow.\n\n## The Challenge: Bringing This to Web Chat\n\nWhen building ChatOllama, we realized that web-based AI chat applications were missing this crucial interaction pattern. Users would:\n\n1. **Read content** in the chat history\n2. **Want to ask** about a specific part\n3. **Copy and paste** the relevant text\n4. **Switch context** to the input field\n5. **Manually explain** what they're referring to\n\nThis workflow breaks the natural flow of conversation. What if we could eliminate steps 3, 4, and 5 entirely?\n\n## Design Goals: Learning from the Best\n\nOur goals were inspired by the best practices we'd seen in AI IDEs:\n\n### 1. **Zero Context Switching**\nThe dialog should appear exactly where the user is reading, not force them to look elsewhere.\n\n### 2. **Perfect Context Preservation**\nThe selected text should remain visually highlighted throughout the interaction.\n\n### 3. **Compact and Focused**\nNo need for full chat interface complexityâ€”just quick, contextual assistance.\n\n### 4. **Model Consistency**\nUse the same AI model as the current session, maintaining conversation continuity.\n\n### 5. **Non-Disruptive**\nNever interfere with the main chat flow or conversation history.\n\n## Technical Architecture: Building for Simplicity\n\n### Component Structure\n\n```typescript\n// Core components\nQuickChat.vue           // Floating dialog UI\nuseQuickChat()          // Chat logic and API communication  \nuseTextSelection()      // Selection handling and preservation\n```\n\n### Key Design Decisions\n\n#### 1. **Floating Dialog with Smart Positioning**\n\n```typescript\nconst dialogStyle = computed(() => {\n  const { x, y } = props.position\n  return {\n    position: 'fixed',\n    top: `${Math.min(y, window.innerHeight - 280)}px`,\n    left: `${Math.min(x, window.innerWidth - 320)}px`,\n    zIndex: 9999,\n    maxWidth: '320px',\n    width: '320px'\n  }\n})\n```\n\nThe dialog appears near the selection but intelligently stays within viewport bounds.\n\n#### 2. **Separate API Strategy**\n\nInstead of mixing quick chat with regular conversation history, we created a dedicated endpoint that:\n\n- Uses the current session's model\n- Bypasses conversation storage\n- Includes selected content as context\n- Returns streaming responses\n\n```typescript\nconst sendQuickChat = async (userQuery: string, selectedContent?: string) => {\n  let systemPrompt = defaultSystemPrompt\n  if (selectedContent) {\n    systemPrompt += `\\n\\nSelected content for context:\\n\"\"\"${selectedContent}\"\"\"`\n  }\n  \n  const messages = [\n    { role: 'system', content: systemPrompt },\n    { role: 'user', content: userQuery }\n  ]\n  \n  // Uses current session's model configuration\n  const response = await fetch('/api/models/chat', {\n    method: 'POST',\n    headers: { ...getKeysHeader() }, // Critical for model consistency\n    body: JSON.stringify({ model, family, messages, stream: true })\n  })\n}\n```\n\n#### 3. **Model Inheritance**\n\nThe trickiest part was ensuring the quick chat used the same model as the current conversation:\n\n```typescript\n// Pass current session models to quick chat\n<QuickChat\n  :current-models=\"models\"\n  :selected-content=\"selectedContent\"\n/>\n\n// Quick chat prioritizes session models\nif (!availableModel && currentModels.value.length > 0) {\n  availableModel = currentModels.value[0] // Use session's first model\n} else if (!availableModel) {\n  availableModel = chatModels.value[0]?.value // Fallback to available models\n}\n```\n\n## The Selection Persistence Challenge\n\nThis turned out to be the most technically challenging aspect of the feature.\n\n### The Problem\n\nWhen a dialog opens and its input field receives focus, browsers automatically clear any existing text selections. This is standard behavior, but it breaks our UX goal of maintaining visual context.\n\n### Failed Approaches\n\n#### 1. **DOM Manipulation**\nOur first attempt wrapped selected text in a `<span>` with highlight CSS. This worked visually but broke the original DOM structure and caused issues with range restoration.\n\n#### 2. **CSS Class on Parent Elements**\nAdding highlight classes to common ancestor elements highlighted entire paragraphs instead of just the selected text.\n\n### The Winning Solution: Aggressive Range Restoration\n\nWe ended up with a multi-layered approach:\n\n```typescript\nexport function useTextSelection() {\n  const savedRange = ref<Range | null>(null)\n  \n  const showQuickChat = (selectionInfo: SelectionInfo) => {\n    // Clone the range to preserve it independently\n    savedRange.value = selectionInfo.range.cloneRange()\n    isQuickChatVisible.value = true\n  }\n  \n  const restoreSelection = () => {\n    if (savedRange.value) {\n      const selection = window.getSelection()\n      if (selection) {\n        selection.removeAllRanges()\n        \n        // Create fresh range to avoid DOM staleness\n        const newRange = document.createRange()\n        newRange.setStart(savedRange.value.startContainer, savedRange.value.startOffset)\n        newRange.setEnd(savedRange.value.endContainer, savedRange.value.endOffset)\n        selection.addRange(newRange)\n      }\n    }\n  }\n}\n```\n\n#### Key Techniques:\n\n1. **Range Cloning**: Clone the selection range immediately to preserve it independently\n2. **Delayed Focus**: Wait 200ms before focusing the input to allow selection restoration\n3. **High-Frequency Monitoring**: Check every 10ms while dialog is open and restore selection if cleared\n4. **Fresh Range Creation**: Create new Range objects to avoid stale DOM references\n\n```typescript\n// In Chat.vue - Aggressive selection maintenance\nwatch(isQuickChatVisible, (visible) => {\n  if (visible) {\n    nextTick(() => {\n      restoreSelection()\n      \n      // Monitor and restore every 10ms\n      const maintainSelection = setInterval(() => {\n        if (isQuickChatVisible.value) {\n          const selection = window.getSelection()\n          if (!selection || selection.rangeCount === 0) {\n            restoreSelection()\n          }\n        } else {\n          clearInterval(maintainSelection)\n        }\n      }, 10)\n    })\n  }\n})\n```\n\n## User Experience Refinements\n\n### Compact UI Design\n\nThe dialog is intentionally small and focused:\n\n```vue\n<template>\n  <div class=\"quick-chat-dialog\" style=\"width: 320px\">\n    <!-- Minimal header with title and close button -->\n    <div class=\"p-3 border-b\">\n      <h3 class=\"text-base font-medium\">Quick Chat</h3>\n    </div>\n    \n    <!-- Compact input area -->\n    <div class=\"p-3\">\n      <textarea rows=\"2\" class=\"text-sm\" />\n      <div class=\"flex justify-between mt-2\">\n        <div class=\"text-xs text-gray-400\">Enter to send, Escape to close</div>\n        <UButton size=\"xs\">Send</UButton>\n      </div>\n    </div>\n    \n    <!-- Response with limited height -->\n    <div class=\"max-h-40 overflow-y-auto p-3\">\n      {{ response }}\n    </div>\n  </div>\n</template>\n```\n\n### Keyboard Interactions\n\n- **Enter**: Send query\n- **Shift + Enter**: New line in input\n- **Escape**: Close dialog\n- **Click outside**: Close dialog\n\n### Streaming Response Display\n\nJust like the main chat, quick chat supports streaming responses with a compact typing indicator:\n\n```vue\n<div v-if=\"isLoading\" class=\"flex items-center mt-2\">\n  <div class=\"flex space-x-1\">\n    <div class=\"w-0.5 h-0.5 bg-current rounded-full animate-bounce\"></div>\n    <div class=\"w-0.5 h-0.5 bg-current rounded-full animate-bounce\" style=\"animation-delay: 0.1s\"></div>\n    <div class=\"w-0.5 h-0.5 bg-current rounded-full animate-bounce\" style=\"animation-delay: 0.2s\"></div>\n  </div>\n</div>\n```\n\n## Development Process: Iterative Excellence\n\n### Phase 1: Proof of Concept\n- Basic dialog that appears on text selection\n- Simple API call without streaming\n- Manual positioning\n\n### Phase 2: Selection Persistence \n- Implemented range saving and restoration\n- Added smart positioning within viewport\n- Introduced compact UI design\n\n### Phase 3: Model Consistency\n- Fixed model inheritance from current session\n- Added proper authentication headers\n- Implemented streaming responses\n\n### Phase 4: Polish and Refinement\n- Refined selection restoration algorithm\n- Added keyboard shortcuts\n- Optimized performance with interval management\n- Added comprehensive error handling\n\n## Technical Challenges and Solutions\n\n### Challenge 1: Browser Selection Behavior\n\n**Problem**: Different browsers handle text selection differently, especially when DOM elements receive focus.\n\n**Solution**: Test across browsers and implement defensive programming with try-catch blocks and multiple restoration strategies.\n\n### Challenge 2: DOM Range Staleness\n\n**Problem**: Saved Range objects can become invalid if the DOM structure changes.\n\n**Solution**: Always create fresh Range objects when restoring, using the saved start/end positions rather than the Range object directly.\n\n### Challenge 3: Performance with High-Frequency Monitoring\n\n**Problem**: Checking selection every 10ms could impact performance.\n\n**Solution**: Only run the interval while the dialog is visible, and clean up immediately when it closes.\n\n### Challenge 4: Viewport Edge Cases\n\n**Problem**: Dialog positioning near viewport edges could cause it to appear off-screen.\n\n**Solution**: Implement smart positioning logic that considers available space and repositions accordingly.\n\n## Architecture Patterns and Best Practices\n\n### 1. Composable-First Design\n\n```typescript\n// Clean separation of concerns\nconst { isQuickChatVisible, selectedContent, setupSelectionHandler } = useTextSelection()\nconst { query, response, sendQuickChat } = useQuickChat()\n```\n\n### 2. Reactive Props for Dynamic Configuration\n\n```typescript\n// Component adapts to changing session state\nconst quickChatOptions = computed(() => ({\n  currentModels: props.currentModels\n}))\n\nconst useQuickChat(quickChatOptions)\n```\n\n### 3. Event-Driven Communication\n\n```vue\n<QuickChat\n  v-model:show=\"isQuickChatVisible\"\n  @close=\"hideQuickChat\"\n  @send=\"handleQuickChatSend\"\n/>\n```\n\n### 4. Defensive Error Handling\n\n```typescript\ntry {\n  const newRange = document.createRange()\n  newRange.setStart(savedRange.value.startContainer, savedRange.value.startOffset)\n  selection.addRange(newRange)\n} catch (e) {\n  console.warn('Failed to restore selection:', e)\n  // Gracefully continue without breaking the UI\n}\n```\n\n## Performance Considerations\n\n### 1. Lazy Loading\n\nQuick chat functionality is only activated when needed:\n\n```typescript\n// Only setup selection handlers after component mounts\nonMounted(() => {\n  setupSelectionHandler(messageListEl.value)\n})\n```\n\n### 2. Efficient Event Management\n\n```typescript\n// Clean up interval when dialog closes\nwatch(isQuickChatVisible, (visible) => {\n  if (visible) {\n    const interval = setInterval(restoreSelection, 10)\n    // Store reference for cleanup\n    onUnmounted(() => clearInterval(interval))\n  }\n})\n```\n\n### 3. Minimal DOM Queries\n\n```typescript\n// Cache DOM references instead of repeated queries\nconst dialogRef = ref<HTMLElement>()\nconst inputRef = ref<HTMLTextAreaElement>()\n```\n\n## Testing Strategy\n\n### Unit Tests for Core Logic\n\n```typescript\ndescribe('useTextSelection', () => {\n  it('should preserve selection range when dialog opens', () => {\n    const { showQuickChat, savedRange } = useTextSelection()\n    const mockRange = createMockRange('selected text')\n    \n    showQuickChat({ selectedText: 'test', range: mockRange })\n    \n    expect(savedRange.value).not.toBeNull()\n    expect(savedRange.value.toString()).toBe('selected text')\n  })\n})\n```\n\n### Integration Tests for User Workflows\n\n```typescript\ndescribe('Quick Chat Workflow', () => {\n  it('should maintain selection throughout interaction', async () => {\n    // Simulate text selection\n    selectText('important text')\n    \n    // Dialog should appear\n    expect(quickChatDialog).toBeVisible()\n    \n    // Selection should still be visible\n    expect(getSelection().toString()).toBe('important text')\n    \n    // After sending query, selection persists\n    await sendQuery('What does this mean?')\n    expect(getSelection().toString()).toBe('important text')\n  })\n})\n```\n\n## Internationalization Support\n\nThe feature includes comprehensive i18n support:\n\n```json\n{\n  \"quickChat\": {\n    \"title\": \"Quick Chat\",\n    \"placeholder\": \"Ask about the selected content...\",\n    \"shortcuts\": \"Enter to send, Escape to close\",\n    \"send\": \"Send\",\n    \"sending\": \"Sending...\",\n    \"thinking\": \"Thinking...\",\n    \"error\": \"An error occurred while processing your request\",\n    \"noModelAvailable\": \"No AI model available. Please configure a model first.\"\n  }\n}\n```\n\nWith Chinese translations:\n\n```json\n{\n  \"quickChat\": {\n    \"title\": \"å¿«é€Ÿå¯¹è¯\",\n    \"placeholder\": \"è¯¢é—®é€‰ä¸­çš„å†…å®¹...\",\n    \"shortcuts\": \"å›žè½¦å‘é€ï¼ŒESC å…³é—­\",\n    \"send\": \"å‘é€\",\n    \"sending\": \"å‘é€ä¸­...\",\n    \"thinking\": \"æ€è€ƒä¸­...\",\n    \"error\": \"å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯\",\n    \"noModelAvailable\": \"æ²¡æœ‰å¯ç”¨çš„AIæ¨¡åž‹ã€‚è¯·å…ˆé…ç½®ä¸€ä¸ªæ¨¡åž‹ã€‚\"\n  }\n}\n```\n\n## Lessons Learned\n\n### 1. User Experience Drives Technical Decisions\n\nThe requirement to maintain visual selection context led us through multiple technical approaches. The UX goal was non-negotiable, so we adapted the technical solution until it worked perfectly.\n\n### 2. Browser APIs Have Subtle Differences\n\nText selection behavior varies across browsers. Testing on Chrome, Firefox, and Safari revealed different edge cases that required defensive programming.\n\n### 3. Simple Features Can Have Complex Implementations\n\nWhat appears to be a \"simple\" dialog actually required:\n- Advanced DOM manipulation\n- Performance optimization\n- Cross-browser compatibility\n- Accessibility considerations\n- Error boundary management\n\n### 4. Inspiration from Other Tools Works\n\nTaking the interaction pattern from AI IDEs and adapting it to web chat worked beautifully. Users immediately understood how to use the feature.\n\n### 5. Iteration Leads to Excellence\n\nEach phase of development revealed new requirements and opportunities for improvement. The final implementation is much more robust than our initial concept.\n\n## Future Enhancements\n\n### 1. Multi-Selection Support\n\nSupport for multiple text selections across different parts of the conversation:\n\n```typescript\ninterface MultiSelection {\n  ranges: Range[]\n  contexts: string[]\n  combinedQuery: string\n}\n```\n\n### 2. Quick Actions Menu\n\nExpand beyond just \"ask\" to include other contextual actions:\n\n```typescript\nconst quickActions = [\n  { id: 'explain', label: 'Explain this', icon: 'lightbulb' },\n  { id: 'translate', label: 'Translate', icon: 'language' },\n  { id: 'summarize', label: 'Summarize', icon: 'document' },\n  { id: 'code-review', label: 'Review code', icon: 'code' }\n]\n```\n\n### 3. Smart Context Detection\n\nAutomatically determine the best context strategy based on selection:\n\n```typescript\nconst contextStrategies = {\n  code: (selection) => ({ language: detectLanguage(selection), type: 'code' }),\n  prose: (selection) => ({ type: 'text', sentiment: detectSentiment(selection) }),\n  data: (selection) => ({ type: 'structured', format: detectFormat(selection) })\n}\n```\n\n### 4. Persistent Quick Chat History\n\nFor power users who want to track their quick queries:\n\n```typescript\ninterface QuickChatHistory {\n  id: string\n  query: string\n  selectedContent: string\n  response: string\n  timestamp: Date\n  sessionId: string\n}\n```\n\n## Impact and Reception\n\nSince implementing this feature, we've observed:\n\n- **Higher engagement** with chat history content\n- **Reduced copy-paste operations** in user workflows  \n- **More contextual questions** being asked\n- **Positive feedback** from users familiar with AI IDEs\n- **Faster iteration** on complex topics\n\nThe feature has proven that bringing familiar interaction patterns from desktop tools to web applications can significantly improve user experience.\n\n## Conclusion\n\nBuilding the contextual quick chat feature taught us that great user experiences often require solving seemingly simple problems in sophisticated ways. The challenge wasn't building a chat dialogâ€”it was preserving text selection across focus changes in web browsers while maintaining perfect model consistency and creating a delightful, non-disruptive interaction.\n\nBy taking inspiration from AI IDEs and adapting their best practices to web applications, we created something that feels both familiar and innovative. The technical complexity is hidden behind a simple, intuitive interface that users immediately understand.\n\nThis project reinforced our belief that the best features are those that feel like natural extensions of existing workflows rather than additional complexity. When users select text and see the quick chat dialog appear exactly where they need it, with their selection still highlighted and the AI ready to help, the feature doesn't feel like a \"feature\" at allâ€”it feels like the way things should work.\n\n---\n\n*This blog post documents the development of the contextual quick chat feature in ChatOllama, implemented on September 11, 2025. The feature brings AI IDE-style contextual assistance to web-based chat applications, solving complex technical challenges around text selection persistence and model consistency.*\n\n**Technical Stack**: Vue 3 + Nuxt 3 + TypeScript + Streaming APIs + Advanced DOM Manipulation\n\n**Project Repository**: [ChatOllama](https://github.com/sugarforever/chat-ollama)\n\n**Feature Components**:\n- `components/QuickChat.vue`\n- `composables/useQuickChat.ts`\n- `composables/useTextSelection.ts`",
      "excerpt": "> How we brought the familiar \"quick edit\" experience from AI IDEs to web-based chat applications, solving text selection persistence and creating a truly contextual AI assistant.\n\n\n\nIf you've used...",
      "readingTime": 12,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250911-building-contextual-quick-chat-inspired-by-ai-ides.md"
    },
    {
      "slug": "20250909-improving-ai-chat-experience-with-smart-title-generation",
      "title": "ç”¨AIä¸ºAIå¯¹è¯ç”Ÿæˆæ ‡é¢˜ï¼šæ”¹è¿›ç”¨æˆ·ä½“éªŒçš„æŠ€æœ¯å®žè·µ",
      "date": "2025-09-09",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# ç”¨AIä¸ºAIå¯¹è¯ç”Ÿæˆæ ‡é¢˜ï¼šæ”¹è¿›ç”¨æˆ·ä½“éªŒçš„æŠ€æœ¯å®žè·µ\n\n> ä¸€ä¸ªçœ‹ä¼¼ç®€å•çš„åŠŸèƒ½ï¼ŒèƒŒåŽçš„æŠ€æœ¯æ€è€ƒå’Œå®žçŽ°ç»†èŠ‚\n\n## èƒŒæ™¯ï¼šä»Žç”¨æˆ·ç—›ç‚¹åˆ°äº§å“æ”¹è¿›\n\nåœ¨AIå¯¹è¯åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸é‡åˆ°è¿™æ ·çš„åœºæ™¯ï¼šç”¨æˆ·å¼€å§‹äº†ä¸€ä¸ªæ–°çš„å¯¹è¯ï¼Œè¯¢é—®å…³äºŽ\"æ„å¤§åˆ©è¶³çƒé’è®­ä½“ç³»\"çš„é—®é¢˜ï¼Œä½†ä¼šè¯åˆ—è¡¨ä¸­æ˜¾ç¤ºçš„å´æ˜¯\"æ–°å¯¹è¯\"æˆ–è€…ä¸€ä¸²æ²¡æœ‰æ„ä¹‰çš„IDã€‚å½“ç”¨æˆ·æƒ³è¦å›žé¡¾ä¹‹å‰çš„å¯¹è¯æ—¶ï¼Œé¢å¯¹ä¸€æŽ’\"æ–°å¯¹è¯\"çš„æ ‡é¢˜ï¼Œåªèƒ½é€ä¸ªç‚¹å‡»æŸ¥çœ‹å†…å®¹ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªå…¸åž‹çš„**ç”¨æˆ·ä½“éªŒå€ºåŠ¡**â€”â€”åŠŸèƒ½å®Œæ•´ï¼Œä½†ç¼ºä¹äººæ€§åŒ–çš„ç»†èŠ‚ã€‚\n\n## äº§å“æ€ç»´ï¼šå°åŠŸèƒ½ï¼Œå¤§ä½“éªŒ\n\n### ç”¨æˆ·æœŸæœ›æ˜¯ä»€ä¹ˆï¼Ÿ\n\n- **å³æ—¶è¯†åˆ«**ï¼šä¸€çœ¼å°±èƒ½çŸ¥é“è¿™ä¸ªå¯¹è¯è®¨è®ºäº†ä»€ä¹ˆ\n- **æ™ºèƒ½ç”Ÿæˆ**ï¼šä¸éœ€è¦æ‰‹åŠ¨è¾“å…¥ï¼Œè‡ªåŠ¨ç†è§£å†…å®¹\n- **å‡†ç¡®ç®€æ´**ï¼šæ ‡é¢˜æ—¢è¦å‡†ç¡®åˆè¦ç®€æ´\n- **å®žæ—¶æ›´æ–°**ï¼šç”ŸæˆåŽç«‹å³åœ¨ç•Œé¢ä¸Šæ˜¾ç¤º\n\n### æŠ€æœ¯æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ\n\nçœ‹èµ·æ¥ç®€å•çš„åŠŸèƒ½ï¼Œå®žé™…å®žçŽ°æ—¶ä¼šé‡åˆ°ä¸å°‘æŒ‘æˆ˜ï¼š\n\n1. **æ¨¡åž‹ä¸€è‡´æ€§**ï¼šæ ‡é¢˜ç”Ÿæˆè¦ä½¿ç”¨å’Œå¯¹è¯ç›¸åŒçš„AIæ¨¡åž‹\n2. **è§¦å‘æ—¶æœº**ï¼šä»€ä¹ˆæ—¶å€™ç”Ÿæˆæ ‡é¢˜ï¼Ÿå¦‚ä½•é¿å…é‡å¤ç”Ÿæˆï¼Ÿ\n3. **æ€§èƒ½è€ƒè™‘**ï¼šä¸èƒ½å½±å“æ­£å¸¸å¯¹è¯çš„å“åº”é€Ÿåº¦\n4. **é”™è¯¯å¤„ç†**ï¼šç”Ÿæˆå¤±è´¥æ—¶å¦‚ä½•ä¼˜é›…é™çº§ï¼Ÿ\n5. **UIåŒæ­¥**ï¼šå¦‚ä½•å®žæ—¶æ›´æ–°ç•Œé¢æ˜¾ç¤ºï¼Ÿ\n\n## æŠ€æœ¯å®žçŽ°ï¼šä»Žç®€å•åˆ°ä¼˜é›…\n\n### ç¬¬ä¸€ç‰ˆï¼šç›´æŽ¥å¤åˆ¶èŠå¤©é€»è¾‘\n\næœ€ç›´è§‚çš„æƒ³æ³•æ˜¯å¤åˆ¶çŽ°æœ‰çš„èŠå¤©APIé€»è¾‘ï¼ŒåŽ»æŽ‰çŸ¥è¯†åº“å’Œæµå¼è¿”å›žï¼š\n\n```typescript\n// ç®€å•ç²—æš´çš„å®žçŽ°\nexport default defineEventHandler(async (event) => {\n  const { model, family, userMessage } = await readBody(event)\n  \n  const llm = createChatModel(model, family, event)\n  const systemPrompt = `ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ ‡é¢˜`\n  \n  const response = await llm.invoke([\n    ['system', systemPrompt],\n    ['user', userMessage]\n  ])\n  \n  return { title: response.content.trim() }\n})\n```\n\nè¿™ä¸ªç‰ˆæœ¬èƒ½å·¥ä½œï¼Œä½†æœ‰å‡ ä¸ªé—®é¢˜ï¼š\n- ç¼ºä¹é…ç½®çµæ´»æ€§\n- é”™è¯¯å¤„ç†ä¸å®Œå–„  \n- æ— æ³•å¤ç”¨åˆ°å…¶ä»–åœºæ™¯\n\n### ç¬¬äºŒç‰ˆï¼šè§£å†³æ¨¡åž‹é…ç½®é—®é¢˜\n\nå®žé™…æµ‹è¯•æ—¶å‘çŽ°ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šèŠå¤©ä½¿ç”¨çš„æ˜¯Moonshotçš„Kimiæ¨¡åž‹ï¼Œä½†æ ‡é¢˜ç”Ÿæˆå´å›žé€€åˆ°äº†æœ¬åœ°Ollamaã€‚\n\n**æ ¹æœ¬åŽŸå› **ï¼šè‡ªå®šä¹‰æ¨¡åž‹é…ç½®æ²¡æœ‰æ­£ç¡®ä¼ é€’åˆ°æ ‡é¢˜ç”ŸæˆAPIã€‚\n\nèŠå¤©APIèƒ½æ­£ç¡®å·¥ä½œæ˜¯å› ä¸ºWeb Workerä¼ é€’äº†å®Œæ•´çš„è¯·æ±‚å¤´ï¼š\n\n```typescript\n// èŠå¤©è¯·æ±‚åŒ…å«å…³é”®çš„é…ç½®ä¿¡æ¯\nconst response = await fetch('/api/models/chat', {\n  method: 'POST',\n  headers: {\n    ...headers, // è¿™é‡ŒåŒ…å«äº† x-chat-ollama-keys\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({...})\n})\n```\n\nè€Œæˆ‘ä»¬çš„æ ‡é¢˜ç”Ÿæˆè¯·æ±‚ç¼ºå°‘äº†è¿™ä¸ªå…³é”®çš„headerï¼š\n\n```typescript\n// ä¿®å¤åŽçš„å®žçŽ°\nconst { getKeysHeader } = await import('~/utils/settings')\n\nconst response = await fetch(`/api/sessions/${sessionId}/title`, {\n  method: 'POST',\n  headers: { \n    'Content-Type': 'application/json',\n    ...getKeysHeader() // å…³é”®ï¼šä¼ é€’æ¨¡åž‹é…ç½®\n  },\n  body: JSON.stringify({ model, family, userMessage })\n})\n```\n\n**æŠ€æœ¯æ´žå¯Ÿ**ï¼šçœ‹ä¼¼ç‹¬ç«‹çš„åŠŸèƒ½ï¼Œå¾€å¾€ä¾èµ–äºŽç³»ç»Ÿçš„åŸºç¡€è®¾æ–½ã€‚ç¡®ä¿æ–°åŠŸèƒ½ä½¿ç”¨ç›¸åŒçš„åŸºç¡€ç»„ä»¶æ˜¯ä¸€è‡´æ€§çš„å…³é”®ã€‚\n\n### ç¬¬ä¸‰ç‰ˆï¼šæ¨¡å—åŒ–é‡æž„\n\néšç€åŠŸèƒ½é€æ¸å®Œå–„ï¼Œæˆ‘æ„è¯†åˆ°ä»£ç è€¦åˆåº¦å¤ªé«˜ï¼Œéš¾ä»¥å¤ç”¨ã€‚äºŽæ˜¯è¿›è¡Œäº†å…¨é¢é‡æž„ï¼š\n\n#### 1. åˆ†å±‚æž¶æž„\n\n```\nComponent Layer    â†’ ä½¿ç”¨è‡ªåŠ¨æ ‡é¢˜ç”Ÿæˆ\n   â†“\nUtility Layer     â†’ é…ç½®è§¦å‘æ¡ä»¶å’Œç­–ç•¥  \n   â†“\nComposable Layer  â†’ æ ¸å¿ƒé€»è¾‘å’ŒAPIè°ƒç”¨\n   â†“\nAPI Layer         â†’ ä¸ŽAIæ¨¡åž‹äº¤äº’\n```\n\n#### 2. èŒè´£åˆ†ç¦»\n\n**Composableå±‚**è´Ÿè´£æ ¸å¿ƒé€»è¾‘ï¼š\n```typescript\nexport function useSessionTitle() {\n  const generateTitleAPI = async (model, family, userMessage, sessionId) => {\n    // çº¯ç²¹çš„APIè°ƒç”¨\n  }\n\n  const updateSessionInDB = async (sessionId, title) => {\n    // çº¯ç²¹çš„æ•°æ®åº“æ“ä½œ\n  }\n\n  const generateSessionTitle = async (options) => {\n    // ç»„åˆAPIè°ƒç”¨å’Œæ•°æ®åº“æ›´æ–°\n  }\n\n  return { generateTitleAPI, updateSessionInDB, generateSessionTitle }\n}\n```\n\n**Utilityå±‚**è´Ÿè´£ç­–ç•¥é…ç½®ï¼š\n```typescript\nexport const titleTriggers = {\n  firstUserMessage: {\n    shouldGenerate: (context) => {\n      // åˆ¤æ–­æ˜¯å¦åº”è¯¥ç”Ÿæˆæ ‡é¢˜çš„é€»è¾‘\n    },\n    extractMessage: (context) => {\n      // æå–ç”¨äºŽç”Ÿæˆæ ‡é¢˜çš„å†…å®¹\n    }\n  }\n}\n```\n\n**Componentå±‚**åªéœ€è¦ç®€å•é…ç½®ï¼š\n```typescript\n// ç»„ä»¶ä¸­çš„ä½¿ç”¨éžå¸¸ç®€æ´\nconst autoTitleGenerator = createAutoTitleGenerator.forFirstMessage((title) => {\n  sessionInfo.value.title = title\n  emit('title-updated', title)\n})\n\n// åœ¨æ¶ˆæ¯å¤„ç†ä¸­è°ƒç”¨\nautoTitleGenerator.attemptTitleGeneration(context, sessionId, model, family)\n```\n\n## è®¾è®¡æ¨¡å¼çš„è¿ç”¨\n\n### 1. ç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰\n\nä¸åŒåœºæ™¯ä¸‹çš„æ ‡é¢˜ç”Ÿæˆç­–ç•¥ï¼š\n\n```typescript\nconst strategies = {\n  firstMessage: { /* é¦–æ¡æ¶ˆæ¯è§¦å‘ */ },\n  onDemand: { /* æŒ‰éœ€ç”Ÿæˆ */ },\n  periodic: { /* å®šæœŸæ›´æ–° */ }\n}\n```\n\n### 2. å·¥åŽ‚æ¨¡å¼ï¼ˆFactory Patternï¼‰\n\nå¿«é€Ÿåˆ›å»ºå¸¸ç”¨é…ç½®ï¼š\n\n```typescript\nconst generator = createAutoTitleGenerator.forFirstMessage(callback)\n// vs å¤æ‚çš„æ‰‹åŠ¨é…ç½®\nconst generator = new AutoTitleGenerator({ \n  enabled: true,\n  trigger: titleTriggers.firstUserMessage,\n  onTitleGenerated: callback \n})\n```\n\n### 3. è§‚å¯Ÿè€…æ¨¡å¼ï¼ˆObserver Patternï¼‰\n\nç»„ä»¶é—´çš„è§£è€¦é€šä¿¡ï¼š\n\n```typescript\n// Chatç»„ä»¶å‘å‡ºäº‹ä»¶\nemit('title-updated', title)\n\n// çˆ¶ç»„ä»¶å“åº”äº‹ä»¶\n@title-updated=\"onTitleUpdated\"\n```\n\n## ç”¨æˆ·ä½“éªŒçš„ç»†èŠ‚\n\n### 1. éžé˜»å¡žè®¾è®¡\n\næ ‡é¢˜ç”Ÿæˆå®Œå…¨å¼‚æ­¥ï¼Œä¸å½±å“æ­£å¸¸å¯¹è¯ï¼š\n\n```typescript\n// å‘é€æ¶ˆæ¯åŽç«‹å³ç»§ç»­ï¼Œæ ‡é¢˜ç”Ÿæˆåœ¨åŽå°è¿›è¡Œ\nemits('message', userMessage)\nmessages.value.push(userMessage)\n\n// å¼‚æ­¥ç”Ÿæˆæ ‡é¢˜ï¼ŒæˆåŠŸåŽæ›´æ–°UI\nautoTitleGenerator.attemptTitleGeneration(...)\n```\n\n### 2. ä¼˜é›…é™çº§\n\nç”Ÿæˆå¤±è´¥æ—¶ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ï¼š\n\n```typescript\ngenerateSessionTitle(options)\n  .then(title => {\n    if (title) updateUI(title)\n  })\n  .catch(error => {\n    console.warn('Title generation failed:', error)\n    // ç”¨æˆ·ä¸ä¼šæ„ŸçŸ¥åˆ°å¤±è´¥ï¼Œå¯¹è¯æ­£å¸¸è¿›è¡Œ\n  })\n```\n\n### 3. å®žæ—¶åé¦ˆ\n\næ ‡é¢˜ç”ŸæˆåŽç«‹å³æ›´æ–°å¤šä¸ªUIä½ç½®ï¼š\n\n```typescript\nconst onTitleGenerated = (title) => {\n  // æ›´æ–°å½“å‰ä¼šè¯æ˜¾ç¤º\n  sessionInfo.value.title = title\n  \n  // æ›´æ–°ä¼šè¯åˆ—è¡¨\n  emit('title-updated', title)\n}\n```\n\n## æŠ€æœ¯äº®ç‚¹ä¸Žåˆ›æ–°\n\n### 1. é…ç½®åŒ–çš„æç¤ºå·¥ç¨‹\n\nä¸åŒåœºæ™¯ä½¿ç”¨ä¸åŒçš„æç¤ºç­–ç•¥ï¼š\n\n```typescript\nconst TITLE_PROMPTS = {\n  concise: (maxWords) => `ç”Ÿæˆ${maxWords}å­—çš„ç®€æ´æ ‡é¢˜`,\n  descriptive: (maxWords) => `ç”Ÿæˆ${maxWords}å­—çš„æè¿°æ€§æ ‡é¢˜`,\n  technical: (maxWords) => `ç”Ÿæˆ${maxWords}å­—çš„æŠ€æœ¯æ€§æ ‡é¢˜`,\n  casual: (maxWords) => `ç”Ÿæˆ${maxWords}å­—çš„è½»æ¾æ ‡é¢˜`\n}\n```\n\n### 2. æ™ºèƒ½å†…å®¹æå–\n\næ”¯æŒå¤šæ¨¡æ€å†…å®¹çš„æ™ºèƒ½æå–ï¼š\n\n```typescript\nextractMessage: (context) => {\n  const content = context.messageContent\n  if (Array.isArray(content)) {\n    // å¤šæ¨¡æ€å†…å®¹ï¼šæå–æ–‡æœ¬éƒ¨åˆ†\n    return content\n      .filter(item => item.type === 'text' && item.text)\n      .map(item => item.text)\n      .join(' ')\n  }\n  // çº¯æ–‡æœ¬å†…å®¹\n  return content\n}\n```\n\n### 3. æ¸è¿›å¼å¢žå¼º\n\nåŠŸèƒ½è®¾è®¡æ”¯æŒæ¸è¿›å¼æ‰©å±•ï¼š\n\n```typescript\n// åŸºç¡€ç”¨æ³•\nconst title = await generateSessionTitle({\n  sessionId, model, family, userMessage\n})\n\n// é«˜çº§ç”¨æ³•\nconst title = await generateSessionTitle({\n  sessionId, model, family, userMessage,\n  style: 'technical',\n  maxWords: 8,\n  onSuccess: (title) => notifyUser(title),\n  onError: (error) => logError(error)\n})\n```\n\n## å¼€å‘è€…ä½“éªŒ\n\n### æ–‡æ¡£å³ä»£ç \n\nä¸ºäº†è®©åŠŸèƒ½çœŸæ­£å¯å¤ç”¨ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å®Œæ•´çš„å¼€å‘è€…æ–‡æ¡£ï¼š\n\n- **ä¸»æ–‡æ¡£**ï¼šå®Œæ•´çš„æž¶æž„è¯´æ˜Žå’Œä½¿ç”¨æŒ‡å—\n- **å¿«é€Ÿå‚è€ƒ**ï¼šå¸¸ç”¨æ¨¡å¼çš„ä»£ç ç‰‡æ®µ  \n- **APIå‚è€ƒ**ï¼šå®Œæ•´çš„TypeScriptç±»åž‹å®šä¹‰\n\n### ç¤ºä¾‹é©±åŠ¨\n\næ–‡æ¡£ä¸­åŒ…å«äº†ä¸°å¯Œçš„å®žé™…ä½¿ç”¨ç¤ºä¾‹ï¼š\n\n```typescript\n// æ–‡æ¡£èŠå¤©åœºæ™¯\nconst generator = createAutoTitleGenerator.forFirstMessage(onTitleGenerated)\n\n// æ–‡æ¡£æ‘˜è¦åœºæ™¯  \nconst title = await generateSessionTitle({\n  sessionId: docId,\n  model: 'gpt-4',\n  family: 'OpenAI',\n  userMessage: documentContent,\n  style: 'descriptive'\n})\n\n// æ‰¹é‡å¤„ç†åœºæ™¯\nconst results = await Promise.allSettled(\n  sessions.map(session => generateTitleAPI(session.model, session.family, session.firstMessage, session.id))\n)\n```\n\n## æ€§èƒ½ä¼˜åŒ–\n\n### 1. æ‡’åŠ è½½\n\né¿å…å¢žåŠ åˆå§‹åŒ…ä½“ç§¯ï¼š\n\n```typescript\n// åŠ¨æ€å¯¼å…¥ï¼Œéœ€è¦æ—¶æ‰åŠ è½½\nimport('~/composables/useSessionTitle').then(({ generateSessionTitle }) => {\n  generateSessionTitle(options)\n})\n```\n\n### 2. é”™è¯¯è¾¹ç•Œ\n\nç¡®ä¿åŠŸèƒ½å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼š\n\n```typescript\ntry {\n  const title = await generateTitleAPI(model, family, userMessage, sessionId)\n  if (title) {\n    await updateSessionInDB(sessionId, title)\n    onSuccess?.(title)\n  }\n} catch (error) {\n  console.warn('Title generation failed:', error)\n  onError?.(error)\n  // ç»§ç»­æ‰§è¡Œï¼Œä¸æŠ›å‡ºå¼‚å¸¸\n}\n```\n\n## æµ‹è¯•ç­–ç•¥\n\n### å•å…ƒæµ‹è¯•\n\næµ‹è¯•æ ¸å¿ƒé€»è¾‘ï¼š\n\n```typescript\ndescribe('Title Triggers', () => {\n  it('should generate on first user message', () => {\n    const context = {\n      messages: [{ role: 'user', content: 'Hello' }],\n      sessionTitle: ''\n    }\n    \n    const shouldGenerate = titleTriggers.firstUserMessage.shouldGenerate(context)\n    expect(shouldGenerate).toBe(true)\n  })\n})\n```\n\n### é›†æˆæµ‹è¯•\n\næµ‹è¯•å®Œæ•´æµç¨‹ï¼š\n\n```typescript\ndescribe('Session Title Generation', () => {\n  it('should generate and save title', async () => {\n    const { generateSessionTitle } = useSessionTitle()\n    \n    const title = await generateSessionTitle({\n      sessionId: 1,\n      model: 'test-model', \n      family: 'OpenAI',\n      userMessage: 'Test message'\n    })\n    \n    expect(title).toBeTruthy()\n  })\n})\n```\n\n## ç»éªŒæ€»ç»“\n\n### 1. ä»Žç”¨æˆ·ä½“éªŒå‡ºå‘\n\næŠ€æœ¯å®žçŽ°è¦æœåŠ¡äºŽç”¨æˆ·ä½“éªŒï¼Œè€Œä¸æ˜¯ç›¸åã€‚\"è‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜\"çœ‹èµ·æ¥æ˜¯æŠ€æœ¯åŠŸèƒ½ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸ºäº†è§£å†³ç”¨æˆ·\"éš¾ä»¥ç®¡ç†å¯¹è¯åŽ†å²\"çš„ç—›ç‚¹ã€‚\n\n### 2. è¿­ä»£å¼å¼€å‘\n\n- **ç¬¬ä¸€ç‰ˆ**ï¼šå¿«é€ŸéªŒè¯æƒ³æ³•å¯è¡Œæ€§\n- **ç¬¬äºŒç‰ˆ**ï¼šè§£å†³å®žé™…éƒ¨ç½²ä¸­çš„é—®é¢˜  \n- **ç¬¬ä¸‰ç‰ˆ**ï¼šä¸ºé•¿æœŸç»´æŠ¤å’Œæ‰©å±•åšå‡†å¤‡\n\n### 3. åŸºç¡€è®¾æ–½çš„é‡è¦æ€§\n\næ–°åŠŸèƒ½å¾€å¾€ä¾èµ–çŽ°æœ‰çš„åŸºç¡€è®¾æ–½ï¼ˆå¦‚è®¤è¯ã€é…ç½®ç®¡ç†ã€é”™è¯¯å¤„ç†ç­‰ï¼‰ã€‚ç¡®ä¿æ–°åŠŸèƒ½å¤ç”¨è¿™äº›åŸºç¡€è®¾æ–½ï¼Œè€Œä¸æ˜¯é‡å¤é€ è½®å­ã€‚\n\n### 4. å¯æµ‹è¯•æ€§è®¾è®¡\n\n- åˆ†ç¦»çº¯å‡½æ•°å’Œå‰¯ä½œç”¨\n- ä¾èµ–æ³¨å…¥è€Œä¸æ˜¯ç¡¬ç¼–ç \n- æä¾›æ¸…æ™°çš„é”™è¯¯è¾¹ç•Œ\n\n### 5. æ–‡æ¡£å³æŠ•èµ„\n\nå®Œå–„çš„æ–‡æ¡£ä¸ä»…å¸®åŠ©ä»–äººç†è§£ä»£ç ï¼Œæ›´é‡è¦çš„æ˜¯ç¡®ä¿åŠŸèƒ½èƒ½è¢«æ­£ç¡®ä½¿ç”¨å’Œæ‰©å±•ã€‚\n\n## æœªæ¥å±•æœ›\n\n### 1. ä¸ªæ€§åŒ–æ ‡é¢˜é£Žæ ¼\n\næ ¹æ®ç”¨æˆ·åå¥½ç”Ÿæˆä¸åŒé£Žæ ¼çš„æ ‡é¢˜ï¼š\n\n```typescript\n// ç”¨æˆ·é…ç½®\nconst userPrefs = {\n  titleStyle: 'technical',    // åå¥½æŠ€æœ¯æ€§è¡¨è¿°\n  titleLength: 'medium',      // ä¸­ç­‰é•¿åº¦\n  language: 'zh'              // ä¸­æ–‡æ ‡é¢˜\n}\n```\n\n### 2. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆ\n\nç»“åˆå¯¹è¯åŽ†å²ç”Ÿæˆæ›´ç²¾å‡†çš„æ ‡é¢˜ï¼š\n\n```typescript\ngenerateContextAwareTitle({\n  currentMessage: \"å…·ä½“é—®é¢˜\",\n  conversationHistory: previousMessages,\n  userProfile: userInterests\n})\n```\n\n### 3. å¤šè¯­è¨€æ”¯æŒ\n\nåŸºäºŽç”¨æˆ·æ¶ˆæ¯è¯­è¨€è‡ªåŠ¨é€‰æ‹©æ ‡é¢˜è¯­è¨€ï¼š\n\n```typescript\nconst detectedLanguage = detectLanguage(userMessage)\nconst title = await generateSessionTitle({\n  ...options,\n  language: detectedLanguage\n})\n```\n\n### 4. æ‰¹é‡ä¼˜åŒ–\n\nä¸ºå·²æœ‰çš„å¤§é‡\"æ— æ ‡é¢˜\"å¯¹è¯æ‰¹é‡ç”Ÿæˆæ ‡é¢˜ï¼š\n\n```typescript\nconst batchGenerateService = new BatchTitleGenerator({\n  concurrency: 5,\n  rateLimiting: true,\n  progressCallback: (progress) => updateUI(progress)\n})\n\nawait batchGenerateService.processExistingSessions()\n```\n\n## ç»“è¯­\n\nä¸€ä¸ª\"ç®€å•\"çš„æ ‡é¢˜ç”ŸæˆåŠŸèƒ½ï¼ŒèƒŒåŽæ¶‰åŠäº†äº§å“è®¾è®¡ã€ç³»ç»Ÿæž¶æž„ã€æ€§èƒ½ä¼˜åŒ–ã€ç”¨æˆ·ä½“éªŒç­‰å¤šä¸ªæ–¹é¢ã€‚è¿™ä¸ªé¡¹ç›®è®©æˆ‘ä»¬çœ‹åˆ°ï¼š\n\n- **ç»†èŠ‚å†³å®šä½“éªŒ**ï¼šå°åŠŸèƒ½ä¹Ÿèƒ½å¸¦æ¥å¤§çš„ç”¨æˆ·ä½“éªŒæå‡\n- **æŠ€æœ¯æœåŠ¡äº§å“**ï¼šæŠ€æœ¯å®žçŽ°è¦ä»¥ç”¨æˆ·éœ€æ±‚ä¸ºå¯¼å‘\n- **æž¶æž„è€ƒé‡é•¿è¿œ**ï¼šä¸ºæœªæ¥çš„æ‰©å±•å’Œç»´æŠ¤åšå¥½å‡†å¤‡\n- **æ–‡æ¡£åŠ©åŠ›åä½œ**ï¼šè‰¯å¥½çš„æ–‡æ¡£è®©åŠŸèƒ½çœŸæ­£å¯å¤ç”¨\n\nåœ¨AIåº”ç”¨å¿«é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œæˆ‘ä»¬ä¸ä»…è¦å…³æ³¨AIèƒ½åŠ›æœ¬èº«ï¼Œæ›´è¦å…³æ³¨å¦‚ä½•è®©è¿™äº›èƒ½åŠ›æ›´å¥½åœ°æœåŠ¡ç”¨æˆ·ï¼Œåˆ›é€ çœŸæ­£æœ‰ä»·å€¼çš„äº§å“ä½“éªŒã€‚\n\n---\n\n*æœ¬æ–‡åŸºäºŽChatOllamaé¡¹ç›®ä¸­è‡ªåŠ¨æ ‡é¢˜ç”ŸæˆåŠŸèƒ½çš„å®žé™…å¼€å‘ç»éªŒæ€»ç»“è€Œæˆã€‚ç›¸å…³ä»£ç å’Œæ–‡æ¡£å·²å¼€æºï¼Œæ¬¢è¿Žå‚è€ƒå’Œè®¨è®ºã€‚*\n\n**æŠ€æœ¯æ ˆ**ï¼šVue3 + Nuxt3 + TypeScript + LangChain + å¤šç§AIæ¨¡åž‹\n\n**é¡¹ç›®åœ°å€**ï¼š[ChatOllama](https://github.com/sugarforever/chat-ollama)\n\n**æ–‡æ¡£è·¯å¾„**ï¼š`docs/guide/session-title-generation.md`",
      "excerpt": "> ä¸€ä¸ªçœ‹ä¼¼ç®€å•çš„åŠŸèƒ½ï¼ŒèƒŒåŽçš„æŠ€æœ¯æ€è€ƒå’Œå®žçŽ°ç»†èŠ‚\n\n\n\nåœ¨AIå¯¹è¯åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸é‡åˆ°è¿™æ ·çš„åœºæ™¯ï¼šç”¨æˆ·å¼€å§‹äº†ä¸€ä¸ªæ–°çš„å¯¹è¯ï¼Œè¯¢é—®å…³äºŽ\"æ„å¤§åˆ©è¶³çƒé’è®­ä½“ç³»\"çš„é—®é¢˜ï¼Œä½†ä¼šè¯åˆ—è¡¨ä¸­æ˜¾ç¤ºçš„å´æ˜¯\"æ–°å¯¹è¯\"æˆ–è€…ä¸€ä¸²æ²¡æœ‰æ„ä¹‰çš„IDã€‚å½“ç”¨æˆ·æƒ³è¦å›žé¡¾ä¹‹å‰çš„å¯¹è¯æ—¶ï¼Œé¢å¯¹ä¸€æŽ’\"æ–°å¯¹è¯\"çš„æ ‡é¢˜ï¼Œåªèƒ½é€ä¸ªç‚¹å‡»æŸ¥çœ‹å†…å®¹ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªå…¸åž‹çš„ç”¨æˆ·ä½“éªŒå€ºåŠ¡â€”â€”åŠŸèƒ½å®Œæ•´ï¼Œä½†ç¼ºä¹äººæ€§åŒ–çš„ç»†èŠ‚ã€‚\n\n\n\n\n\n-...",
      "readingTime": 5,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250909-improving-ai-chat-experience-with-smart-title-generation.md"
    },
    {
      "slug": "20250828-openai-langchain-image-parsing-fix",
      "title": "OpenAI-Compatible Image Parsing: Fixing LangChain Streaming Limitations",
      "date": "2025-08-28",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# OpenAI-Compatible Image Parsing: Fixing LangChain Streaming Limitations\n\n**Date:** August 28, 2025  \n**Issue:** OpenAI-compatible APIs returning images in streaming responses not parsed by LangChain.js  \n**Resolution Time:** ~6 hours  \n\n## ðŸ› The Problem\n\nWhile ChatOllama supported image uploads from users, a critical gap existed in handling AI-generated images from multimodal models. When using OpenAI-compatible APIs (particularly OpenRouter with Gemini models) that return images as part of their responses, these images were completely ignored during streaming chat sessions.\n\nThe issue was particularly problematic for users leveraging advanced multimodal models that could generate charts, diagrams, or other visual content. Instead of seeing the generated images, users would only receive text responses, missing crucial visual information that models like Gemini Flash were producing.\n\nThis limitation significantly impacted the user experience, especially for:\n- Data visualization requests (charts, graphs)  \n- Diagram generation tasks\n- Creative image generation workflows\n- Technical documentation with visual aids\n\n## ðŸ” Root Cause Investigation\n\nAfter extensive debugging and API response analysis, we discovered that OpenAI-compatible providers use a different response structure for image content compared to the standard OpenAI format that LangChain.js expected.\n\n### The Hidden Response Structure\n\nMost OpenAI-compatible APIs (like OpenRouter) return image content using an `images` field alongside the standard `content` field:\n\n```json\n{\n  \"role\": \"assistant\",\n  \"content\": \"Here's the chart you requested: \",\n  \"images\": [\n    {\n      \"type\": \"image_url\",\n      \"image_url\": {\n        \"url\": \"data:image/png;base64,iVBORw0KGgo...\",\n        \"detail\": \"high\"\n      }\n    }\n  ]\n}\n```\n\nHowever, LangChain.js streaming processors only handled these fields:\n- âœ… `content` field (text content)\n- âœ… `tool_calls` field (function calls)  \n- âœ… `function_call` field (legacy function calls)\n- âœ… `audio` field (audio content)\n- âŒ `images` field (**completely ignored**)\n\nThe core issue was in two critical functions within the LangChain OpenAI chat models:\n1. `_convertCompletionsDeltaToBaseMessageChunk()` - For streaming responses\n2. `_convertCompletionsMessageToBaseMessage()` - For non-streaming responses\n\nBoth functions simply discarded any `images` field data, causing visual content to vanish from the final message.\n\n## ðŸ”§ The Fix Implementation\n\n### Step-by-Step Implementation Guide\n\nTo implement this fix in your own project, you'll need to make changes to three key areas:\n\n1. **Custom LangChain OpenAI Chat Model** - Parse `images` field from API responses\n2. **Server Endpoint** - Extract and handle multimodal content \n3. **Frontend Components** - Display parsed images\n\n### Step 1: Create Custom LangChain Implementation\n\nSince this was a fundamental limitation in LangChain.js itself, we created a customized version of the OpenAI chat models at `server/models/openai/chat_models.ts`.\n\n**Required Changes:**\n\n#### 1.1. Enhanced Streaming Delta Processing\n\nFind the `_convertCompletionsDeltaToBaseMessageChunk()` method in your LangChain OpenAI chat model and modify it:\n\n**Before (Original LangChain):**\n```typescript\nconst content = delta.content ?? \"\"\n```\n\n**After (Fixed):**\n```typescript\nlet content = delta.content ?? \"\"\n\n// Handle images field that might contain image_url content\nif (delta.images && Array.isArray(delta.images)) {\n  // Convert content to array format if it's a string and there are images\n  if (typeof content === \"string\") {\n    const contentArray = []\n    if (content) {\n      contentArray.push({ type: \"text\", text: content })\n    }\n    // Add image content from the images field\n    for (const image of delta.images) {\n      if (image.type === \"image_url\" && image.image_url) {\n        contentArray.push({\n          type: \"image_url\",\n          image_url: image.image_url,\n        })\n      }\n    }\n    content = contentArray\n  }\n}\n```\n\n#### 1.2. Enhanced Non-Streaming Message Processing  \n\nFind the `_convertCompletionsMessageToBaseMessage()` method and modify it:\n\n**Before (Original LangChain):**\n```typescript\nreturn new AIMessage({\n  content: message.content || \"\",\n  // ... other fields\n})\n```\n\n**After (Fixed):**\n```typescript\n// Handle images field that might contain image_url content\nlet content = message.content || \"\"\nif (message.images && Array.isArray(message.images)) {\n  // Convert content to array format if it's a string and there are images\n  if (typeof content === \"string\") {\n    const contentArray = []\n    if (content) {\n      contentArray.push({ type: \"text\", text: content })\n    }\n    // Add image content from the images field\n    for (const image of message.images) {\n      if (image.type === \"image_url\" && image.image_url) {\n        contentArray.push({\n          type: \"image_url\",\n          image_url: image.image_url,\n        })\n      }\n    }\n    content = contentArray\n  }\n}\n\nreturn new AIMessage({\n  content,\n  // ... other fields\n})\n```\n\n### Step 2: Update Server Endpoint Content Processing\n\nModify your chat endpoint to extract and handle multimodal content from the enhanced LangChain implementation:\n\n**File:** `server/api/models/chat/index.post.ts` (or your equivalent)\n\n**Add this new function:**\n```typescript\nconst extractContentFromChunk = (chunk: BaseMessageChunk): { text: string; images: any[] } => {\n  let content = chunk?.content\n  let textContent = ''\n  let images: any[] = []\n\n  // Handle array content (multimodal)\n  if (Array.isArray(content)) {\n    // Extract text content\n    textContent = content\n      .filter(item => item.type === 'text_delta' || item.type === 'text')\n      .map(item => ('text' in item ? item.text : ''))\n      .join('')\n\n    // Extract image content\n    images = content\n      .filter(item => item.type === 'image_url' && item.image_url?.url)\n      .map(item => ({ type: 'image_url', image_url: item.image_url }))\n  } else {\n    // Handle string content\n    textContent = content || ''\n  }\n\n  return { text: textContent, images }\n}\n```\n\n**Update your streaming logic:**\n```typescript\n// Replace existing extractContentFromChunk calls\nconst { text, images } = extractContentFromChunk(chunk)\n\n// Handle both text and images in your response\nif (accumulatedImages.length > 0) {\n  const contentArray: MessageContent[] = []\n  if (accumulatedTextContent) {\n    contentArray.push({ type: 'text', text: accumulatedTextContent })\n  }\n  contentArray.push(...accumulatedImages)\n  contentToStream = contentArray\n} else {\n  contentToStream = accumulatedTextContent\n}\n```\n\n### Step 3: Frontend Image Display Implementation\n\nEnsure your frontend components can extract and display images from the multimodal content:\n\n**File:** `components/ChatMessageItem.vue` (or your equivalent)\n\n**Add image extraction logic:**\n```typescript\nconst messageImages = computed(() => {\n  const content = props.message.content\n  if (!content || !Array.isArray(content)) return []\n\n  return content\n    .filter(item => item.type === 'image_url' && item.image_url?.url)\n    .map(item => item.image_url!.url)\n})\n```\n\n**Update your template to display images:**\n```vue\n<template>\n  <!-- Text content -->\n  <div v-if=\"messageContent\" v-html=\"markdown.render(messageContent)\" />\n  \n  <!-- Image gallery -->\n  <div v-if=\"messageImages.length > 0\" class=\"image-gallery\">\n    <img v-for=\"(url, index) in messageImages\"\n         :key=\"index\"\n         :src=\"url\"\n         :alt=\"`Image ${index + 1}`\"\n         class=\"rounded-lg max-h-64 object-contain\" />\n  </div>\n</template>\n```\n\n**Add basic CSS for image display:**\n```css\n.image-gallery {\n  display: grid;\n  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n  gap: 0.5rem;\n  margin-top: 0.75rem;\n}\n\n.image-gallery img {\n  width: 100%;\n  height: auto;\n  background: var(--color-gray-100);\n  cursor: pointer;\n}\n```\n\n## ðŸ§ª Comprehensive Testing Strategy\n\nWe implemented extensive testing to ensure robustness across different scenarios:\n\n**Test Coverage:**\n1. âœ… **Text with single image** - Proper array conversion\n2. âœ… **Multiple images** - Maintains correct order and structure  \n3. âœ… **Images only (empty content)** - Works without text content\n4. âœ… **Backward compatibility** - No breaking changes for standard responses\n5. âœ… **Invalid image objects** - Graceful error handling\n6. âœ… **Empty images array** - Handles edge cases properly\n7. âœ… **Malformed data** - Robust error handling for invalid inputs\n\n**Validation Commands:**\n```bash\nnpx tsx server/models/openai/tests/validate-core-logic.ts\nnpx tsx server/models/openai/tests/validate-image-url-parsing.ts\n```\n\n## ðŸŽ¯ Content Format Transformation\n\nThe fix intelligently transforms API responses into LangChain-compatible multimodal content:\n\n### Input (OpenAI-Compatible API):\n```json\n{\n  \"content\": \"Here are two visualizations: \",\n  \"images\": [\n    { \n      \"type\": \"image_url\", \n      \"image_url\": { \"url\": \"data:image/png;base64,chart1...\" } \n    },\n    { \n      \"type\": \"image_url\", \n      \"image_url\": { \"url\": \"data:image/png;base64,chart2...\" } \n    }\n  ]\n}\n```\n\n### Output (LangChain Message):\n```json\n[\n  { \"type\": \"text\", \"text\": \"Here are two visualizations: \" },\n  { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/png;base64,chart1...\" } },\n  { \"type\": \"image_url\", \"image_url\": { \"url\": \"data:image/png;base64,chart2...\" } }\n]\n```\n\n## ðŸ“š Lessons Learned\n\nThis implementation taught us several valuable lessons about working with evolving AI APIs:\n\n**API Standardization is Still Evolving:** Different OpenAI-compatible providers use varying response formats for multimodal content. Being adaptable to these differences is crucial for maintaining broad compatibility.\n\n**Custom LangChain Implementations Have Value:** While staying close to upstream LangChain is generally preferred, sometimes specific use cases require custom implementations to unlock functionality that standard libraries don't yet support.\n\n**Robust Testing Prevents Regressions:** Comprehensive edge case testing was essential, especially when dealing with the variety of response formats from different API providers.\n\n**Backward Compatibility is Non-Negotiable:** Any changes to core message processing must maintain 100% backward compatibility to avoid breaking existing workflows.\n\n## ðŸš€ Impact and Results\n\nThe implementation delivers significant improvements to ChatOllama's multimodal capabilities:\n\n**Immediate Benefits:**\n- **Full Multimodal Support**: Users can now see AI-generated images from models like Gemini Flash\n- **Enhanced Visualizations**: Data charts, diagrams, and creative images display properly  \n- **API Provider Flexibility**: Works seamlessly with OpenRouter, OpenAI, and other compatible providers\n- **Zero Breaking Changes**: Existing text-only workflows remain completely unaffected\n\n**Technical Improvements:**\n- **Streaming Performance**: Images appear in real-time as they're generated\n- **Memory Efficiency**: Optimized processing only activates when images are present\n- **Error Resilience**: Graceful handling of malformed or incomplete image data\n- **Future-Proof Architecture**: Ready for additional multimodal content types\n\n## ðŸ’¡ Real-World Usage Examples\n\nThis fix enables powerful new workflows:\n\n```typescript\n// User Request: \"Create a bar chart showing Q4 sales data\"\n// API Response: Mixed text + generated image\n{\n  \"role\": \"assistant\", \n  \"content\": \"Here's your Q4 sales visualization:\",\n  \"images\": [{\n    \"type\": \"image_url\",\n    \"image_url\": {\n      \"url\": \"data:image/png;base64,<chart_data>\",\n      \"detail\": \"high\"\n    }\n  }]\n}\n\n// ChatOllama Now Displays: Text + Interactive Image\n```\n\n## ðŸš€ Quick Implementation Checklist\n\nFor developers implementing this fix:\n\n### âœ… **Required Files to Modify:**\n\n1. **`server/models/openai/chat_models.ts`** (or copy from `@langchain/openai`)\n   - âœ… Add image parsing to `_convertCompletionsDeltaToBaseMessageChunk()` \n   - âœ… Add image parsing to `_convertCompletionsMessageToBaseMessage()`\n\n2. **`server/api/models/chat/index.post.ts`** (your chat endpoint)\n   - âœ… Update `extractContentFromChunk()` function\n   - âœ… Handle multimodal content in streaming logic\n\n3. **`components/ChatMessageItem.vue`** (your message component)\n   - âœ… Add `messageImages` computed property\n   - âœ… Update template with image gallery\n   - âœ… Add CSS for image display\n\n### âœ… **Key Code Patterns to Look For:**\n\n**Problem Indicators:**\n```typescript\n// âŒ Only handles text content\nconst content = delta.content ?? \"\"\n\n// âŒ Ignores images field completely  \nreturn new AIMessage({ content: message.content })\n```\n\n**Solution Patterns:**\n```typescript\n// âœ… Handles both text and images\nif (delta.images && Array.isArray(delta.images)) {\n  // Convert to multimodal array format\n}\n\n// âœ… Extracts images from multimodal content\nreturn content\n  .filter(item => item.type === 'image_url' && item.image_url?.url)\n  .map(item => item.image_url!.url)\n```\n\n### âœ… **Testing Your Implementation:**\n\n1. **Test with OpenRouter + Gemini Flash** (known to return `images` field)\n2. **Verify both streaming and non-streaming responses**  \n3. **Check multiple images in single response**\n4. **Ensure backward compatibility with text-only responses**\n\n---\n\n*This fix enables full multimodal support for OpenAI-compatible APIs that use the `images` response field. By implementing these three key changes, you can unlock image generation capabilities in your LangChain.js-based chat applications.*",
      "excerpt": "Date: August 28, 2025  \nIssue: OpenAI-compatible APIs returning images in streaming responses not parsed by LangChain.js  \nResolution Time: ~6 hours  \n\n\n\nWhile ChatOllama supported image uploads from...",
      "readingTime": 9,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250828-openai-langchain-image-parsing-fix.md"
    },
    {
      "slug": "20250826-model-api-refactoring-parallel-execution",
      "title": "Model API Refactoring: Implementing Parallel Execution and Proper Gemini Integration",
      "date": "2025-08-26",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# Model API Refactoring: Implementing Parallel Execution and Proper Gemini Integration\n\n*August 26, 2025*\n\n## The Challenge\n\nLike many fast-moving projects, ChatOllama started with a pragmatic approach to model management. In the early stages, we simply hardcoded the supported models for different AI provider families in static arrays. This was a conscious decision to move fast and get the core functionality working quickly - a classic \"make it work first, optimize later\" approach.\n\nHowever, as the AI landscape evolved rapidly and our platform matured, this technical debt started to create real problems:\n\n**Outdated Model Lists**: New models from providers like OpenAI, Gemini, and others weren't immediately available to users. We had to manually update our static lists every time providers released new capabilities.\n\n**Maintenance Overhead**: Each provider update meant code changes, testing, and deployment cycles just to keep our model lists current.\n\n**User Frustration**: Power users who wanted to try the latest models (like GPT-4 Turbo variants or new Gemini models) had to wait for us to update our hardcoded lists.\n\n**Performance Issues**: On top of the maintenance burden, we discovered that our model discovery API had architectural limitations affecting performance. The existing implementation was processing external API calls sequentially, and our Gemini API integration wasn't aligned with the actual API response schema.\n\nIt was time to address this technical debt properly and build a more dynamic, sustainable solution.\n\n## What We Discovered\n\nDuring our analysis, we identified several key issues:\n\n1. **Sequential API Processing**: The existing code was making API calls to different providers (OpenAI, Gemini, custom endpoints) one after another, creating unnecessary latency when multiple providers were configured.\n\n2. **Incorrect Gemini API Schema**: Our interface definition didn't match the actual Gemini API response structure, which includes a `models` array and optional `nextPageToken` for pagination support.\n\n3. **Monolithic Function**: All the model fetching logic was embedded in a single event handler, making it difficult to maintain and extend for new providers.\n\n4. **Incomplete Field Utilization**: We were defining interface fields that the application didn't actually need, leading to unnecessary data processing.\n\n## The Solution: Parallel Execution and Modular Design\n\nWe approached this refactoring with three main goals: improve performance through parallelization, ensure API accuracy, and enhance maintainability through modular design.\n\n### 1. Extracting Provider-Specific Functions\n\nFirst, we broke down the monolithic approach into dedicated functions for each provider:\n\n```typescript\n// Fetch OpenAI models\nasync function fetchOpenAIModels(apiKey: string): Promise<ModelItem[]> {\n  try {\n    const response = await fetch('https://api.openai.com/v1/models', {\n      headers: {\n        'Authorization': `Bearer ${apiKey}`,\n      }\n    })\n    // ... processing logic with fallback\n  } catch (error) {\n    console.error('Failed to fetch OpenAI models:', error)\n  }\n  // Fallback to static models\n  return OPENAI_GPT_MODELS.map((model) => ({\n    name: model,\n    details: { family: MODEL_FAMILIES.openai }\n  }))\n}\n```\n\nThis pattern was replicated for `fetchGeminiModels()`, `fetchOllamaModels()`, and `fetchCustomModels()`, giving each provider its own isolated logic while maintaining consistent error handling and fallback mechanisms.\n\n### 2. Implementing Parallel Execution\n\nThe real performance breakthrough came from implementing parallel execution using `Promise.allSettled()`:\n\n```typescript\nexport default defineEventHandler(async (event) => {\n  const keys = event.context.keys\n  const models: ModelItem[] = []\n\n  // Prepare parallel API calls for providers that support dynamic fetching\n  const apiCalls: Promise<ModelItem[]>[] = []\n  \n  // Always try to fetch Ollama models\n  apiCalls.push(fetchOllamaModels(event))\n  \n  // Add API calls based on available keys\n  if (keys.openai.key) {\n    apiCalls.push(fetchOpenAIModels(keys.openai.key))\n  }\n  \n  if (keys.gemini.key) {\n    apiCalls.push(fetchGeminiModels(keys.gemini.key))\n  }\n  \n  // Execute all API calls in parallel\n  const results = await Promise.allSettled(apiCalls)\n  \n  // Process results gracefully\n  results.forEach((result) => {\n    if (result.status === 'fulfilled') {\n      models.push(...result.value)\n    } else {\n      console.error('Failed to fetch models:', result.reason)\n    }\n  })\n  \n  // ... continue with static providers\n})\n```\n\nThis approach ensures that if you have both OpenAI and Gemini API keys configured, both APIs are called simultaneously rather than sequentially, significantly reducing the total response time.\n\n### 3. Correcting the Gemini API Integration\n\nWe updated our Gemini API integration to match the actual response schema:\n\n```typescript\n// Updated interface matching actual Gemini API\ninterface GeminiModelApiResponse {\n  models: Array<{\n    name: string\n    displayName?: string\n    description?: string\n    supportedGenerationMethods?: string[]\n  }>\n  nextPageToken?: string\n}\n\n// Proper API call implementation\nasync function fetchGeminiModels(apiKey: string): Promise<ModelItem[]> {\n  try {\n    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models?key=${apiKey}`)\n\n    if (response.ok) {\n      const data: GeminiModelApiResponse = await response.json()\n      return data.models\n        .filter(model => \n          model.supportedGenerationMethods?.includes('generateContent') &&\n          !model.name.includes('embedding')\n        )\n        .map(model => ({\n          name: model.name.replace('models/', ''), // Remove API prefix\n          details: {\n            family: MODEL_FAMILIES.gemini\n          }\n        }))\n    }\n  } catch (error) {\n    console.error('Failed to fetch Gemini models:', error)\n  }\n  \n  // Fallback to static models\n  return GEMINI_MODELS.map((model) => ({\n    name: model,\n    details: {\n      family: MODEL_FAMILIES.gemini\n    }\n  }))\n}\n```\n\nWe also ensured that only the fields actually needed by the application are included in our interface definition, optimizing both memory usage and type safety.\n\n## The Results\n\nThe refactoring delivered several tangible improvements:\n\n**Performance Enhancement**: Users with multiple API providers configured now experience significantly faster model loading, as API calls execute in parallel rather than sequentially.\n\n**Better Error Resilience**: Using `Promise.allSettled()` means that if one provider's API fails, others continue to work normally, providing a more robust user experience.\n\n**Enhanced Maintainability**: The modular approach makes it much easier to add new AI providers or modify existing integrations without affecting other parts of the system.\n\n**Accurate Data Integration**: The corrected Gemini API integration ensures we're getting the most up-to-date model information directly from Google's API, rather than relying solely on static lists.\n\n**Future-Ready Architecture**: The inclusion of `nextPageToken` support means we're prepared for pagination if needed in the future, and the modular design makes extending functionality straightforward.\n\n## Reflecting on Technical Debt\n\nThis refactoring is a perfect example of how early pragmatic decisions can evolve into technical debt over time. The initial choice to hardcode model lists was absolutely the right decision for rapid prototyping and early development. It allowed us to focus on core features without getting bogged down in API integration complexity.\n\nHowever, the AI landscape moves incredibly fast. What seemed like a manageable list of models quickly became a maintenance burden as providers like OpenAI and Google released new models monthly, sometimes weekly. The static approach that served us well in the beginning became a bottleneck for user experience and developer productivity.\n\nThe key lesson here is recognizing when technical debt has grown from \"helpful shortcut\" to \"user-impacting limitation.\" The transition point came when we realized users were asking for models that existed in the wild but weren't available in our platform due to our hardcoded lists.\n\n## Technical Lessons Learned\n\nThis refactoring reinforced several important principles for API integration and technical debt management:\n\n1. **Parallel Over Sequential**: When dealing with multiple independent external APIs, always consider parallel execution to improve user experience.\n\n2. **Accuracy Over Assumption**: Always verify actual API response schemas rather than making assumptions based on documentation or other sources.\n\n3. **Modular Design**: Breaking down complex operations into focused, single-responsibility functions improves both maintainability and testability.\n\n4. **Graceful Degradation**: Each integration should have proper fallback mechanisms to ensure the application remains functional even when external services are unavailable.\n\n5. **Interface Minimalism**: Only include the data fields your application actually needs to optimize performance and maintain clean code.\n\n6. **Technical Debt Recognition**: Hardcoded solutions can be valuable for rapid development, but establishing clear criteria for when to transition to dynamic approaches prevents user-impacting limitations.\n\n7. **Progressive Enhancement**: The new dynamic system maintains static fallbacks, ensuring reliability while providing the benefits of real-time data.\n\n## Looking Forward\n\nThis refactoring sets a solid foundation for future enhancements to our model management system. The modular architecture makes it straightforward to add support for new AI providers, and the parallel execution pattern can be applied to other areas of the application where multiple external API calls are needed.\n\nThe improved Gemini integration also opens up opportunities to leverage additional features from Google's Generative AI API as they become available, while maintaining the performance benefits of our new parallel execution approach.\n\n---\n\n*This improvement is part of our ongoing effort to optimize ChatOllama's performance and maintainability. For more technical insights and updates, follow our development blog series.*\n",
      "excerpt": "August 26, 2025\n\n\n\nLike many fast-moving projects, ChatOllama started with a pragmatic approach to model management. In the early stages, we simply hardcoded the supported models for different AI...",
      "readingTime": 7,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250826-model-api-refactoring-parallel-execution.md"
    },
    {
      "slug": "20250825-docker-langchain-module-resolution-fix",
      "title": "Fixing Docker Module Resolution Error: LangChain Dependencies Investigation",
      "date": "2025-08-25",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# Fixing Docker Module Resolution Error: LangChain Dependencies Investigation\n\n**Date**: August 25, 2025  \n**Issue**: Docker container failing with `Cannot find module '@langchain/core/prompts.js'` error  \n**Solution**: Dependency version alignment across LangChain packages  \n\n## Problem Description\n\nThe dockerized ChatOllama application was experiencing critical module resolution errors during chat operations:\n\n```\n[nuxt] [request error] [unhandled] [500] Cannot find module '/app/.output/server/node_modules/@langchain/core/prompts.js' \nimported from /app/.output/server/chunks/routes/api/models/index.post.mjs\n```\n\nThis error occurred consistently across multiple API endpoints (`/api/models/chat`, `/api/instruction`, `/api/agents`) and prevented the application from functioning properly in Docker containers.\n\n## Investigation Process\n\n### 1. Initial Analysis\n- **Error Pattern**: ESM module resolution failure for `@langchain/core/prompts.js`\n- **Environment**: Docker container build process, not local development\n- **Affected Files**: Server API routes importing from `@langchain/core/prompts`\n\n### 2. Container Inspection\nInvestigation revealed missing export files in the Docker container:\n\n```bash\n# Expected but missing\n/app/.output/server/node_modules/@langchain/core/prompts.js\n\n# Available directory structure\n/app/.output/server/node_modules/@langchain/core/dist/prompts/index.js\n```\n\n### 3. Version Conflict Discovery\nFound **three different versions** of `@langchain/core` in the dependency tree:\n\n- **Project specification**: `@langchain/core@^0.3.49`\n- **Actual Docker resolution**: `@langchain/core@0.3.72` (pulled by `deepagents@0.0.1`)  \n- **Legacy versions**: `@langchain/core@0.1.54` (used by older packages)\n\nThe key issue: `deepagents@0.0.1` dependency was forcing `@langchain/core@0.3.72`, while the project specified `^0.3.49`, creating version conflicts during Nuxt's build bundling process.\n\n## Root Cause Analysis\n\n### Core Issue\n**Version Mismatch**: The newer `@langchain/core@0.3.72` has different export structures that weren't compatible with how Nuxt bundled the modules for Docker deployment.\n\n### Why Docker vs Local?\n- **Local Development**: pnpm's workspace resolution handled conflicts gracefully\n- **Docker Build**: Nuxt's production bundling exposed the version inconsistencies\n- **Module Resolution**: Different ESM export mappings between versions\n\n### Technical Details\n```json\n// package.json specified\n\"@langchain/core\": \"^0.3.49\"\n\n// But dependency resolution pulled\n\"deepagents@0.0.1\" â†’ \"@langchain/core@0.3.72\"\n\n// Resulted in missing exports during bundling\n```\n\n## Solution: Dependency Alignment\n\n### Approach\nInstead of manual file patching, we chose **proper dependency management** by updating all LangChain packages to compatible versions.\n\n### Package Updates Applied\n\n```json\n{\n  // Core updates for version alignment\n  \"@langchain/core\": \"^0.3.49\" â†’ \"^0.3.72\",\n  \n  // Compatible package updates\n  \"@langchain/anthropic\": \"^0.3.19\" â†’ \"^0.3.26\",\n  \"@langchain/community\": \"^0.3.41\" â†’ \"^0.3.53\", \n  \"@langchain/google-genai\": \"^0.1.5\" â†’ \"^0.2.16\",\n  \"@langchain/groq\": \"^0.0.5\" â†’ \"^0.2.3\",\n  \"@langchain/ollama\": \"^0.2.0\" â†’ \"^0.2.3\",\n  \"@langchain/openai\": \"^0.5.7\" â†’ \"^0.6.9\",\n  \n  // Provider-specific updates\n  \"@langchain/azure-openai\": \"^0.0.4\" â†’ \"^0.0.11\",\n  \"@langchain/cohere\": \"^0.0.6\" â†’ \"^0.3.4\",\n  \n  // Peer dependency fixes\n  \"ws\": \"^8.16.0\" â†’ \"^8.18.0\",\n  \"zod\": \"^3.23.8\" â†’ \"^3.24.1\"\n}\n```\n\n### Implementation Steps\n\n```bash\n# 1. Update package.json with compatible versions\n# 2. Reinstall dependencies\npnpm install\n\n# 3. Verify build success\npnpm run build\n\n# 4. Fix discovered syntax error\n# (Missing parenthesis in server/api/agents/[id].post.ts)\n\n# 5. Successful build completion\nâœ“ Built in 17.34s\n```\n\n## Verification Results\n\n### Before Fix\n- **Docker Error**: Module resolution failure\n- **Version Conflicts**: 3 different @langchain/core versions\n- **Peer Dependencies**: Multiple warnings\n- **Build Status**: Failed in Docker\n\n### After Fix  \n- **Dependency Resolution**: All LangChain packages using `@langchain/core@0.3.72`\n- **Local Build**: âœ… Successful (`pnpm run build`)\n- **Module Exports**: Consistent across all packages\n- **Peer Warnings**: Reduced to minimal non-critical issues\n\n## Best Practices Learned\n\n### 1. Dependency Management\n- **Always align major dependency versions** across related package families\n- **Use exact or compatible ranges** for critical dependencies like LangChain core\n- **Regular dependency audits** to catch version drift\n\n### 2. Docker-Specific Considerations\n- **Test builds in Docker** during development, not just locally\n- **Version conflicts manifest differently** in containerized builds vs local development\n- **ESM module resolution** can be sensitive to version mismatches\n\n### 3. Investigation Approach\n- **Container inspection first** to understand actual file structure\n- **Dependency tree analysis** to identify version conflicts  \n- **Standard tooling over manual fixes** for sustainable solutions\n\n## Technical Details for Developers\n\n### Files Modified\n- `package.json`: Updated LangChain package versions\n- `pnpm-lock.yaml`: Regenerated with consistent resolutions\n- `server/api/agents/[id].post.ts`: Fixed syntax error (missing parenthesis)\n\n### Commands for Reproduction\n```bash\n# Inspect container dependencies\ndocker exec <container> ls -la /app/.output/server/node_modules/@langchain/core/\n\n# Check for missing exports\ndocker exec <container> find /app/.output/server/node_modules/@langchain/core -name \"*prompt*\"\n\n# Verify local vs container differences\nnpm list @langchain/core\n```\n\n### Prevention Strategy\n```json\n// package.json - Use stricter version ranges for critical deps\n{\n  \"@langchain/core\": \"~0.3.72\",  // Tilde for patch-level only\n  \"deepagents\": \"^0.0.1\"         // Ensure compatibility\n}\n```\n\n## Conclusion\n\nThis issue highlights the importance of **consistent dependency management** in modern JavaScript applications, especially when deploying via Docker. The proper solution involved updating the entire LangChain ecosystem to compatible versions rather than applying manual patches.\n\n### Key Takeaways\n1. **Version conflicts** can manifest differently between local and Docker environments\n2. **Dependency alignment** is crucial for ESM module resolution\n3. **Standard package management** is always preferable to manual file fixes\n4. **Container-specific testing** should be part of the development workflow\n\nThe fix ensures ChatOllama's Docker deployment works reliably while maintaining the standard build process and keeping dependencies up-to-date with the latest LangChain ecosystem improvements.",
      "excerpt": "Date: August 25, 2025  \nIssue: Docker container failing with Cannot find module '@langchain/core/prompts.js' error  \nSolution: Dependency version alignment across LangChain packages  \n\n\n\nThe...",
      "readingTime": 4,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250825-docker-langchain-module-resolution-fix.md"
    },
    {
      "slug": "20250825-langchain-upgrade-chat-fix",
      "title": "LangChain Core Package Upgrade Breaks Chat: A Quick Fix Story",
      "date": "2025-08-25",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# LangChain Core Package Upgrade Breaks Chat: A Quick Fix Story\n\n**Date:** August 25, 2025  \n**Issue:** Chat functionality broken after LangChain dependency upgrade  \n**Resolution Time:** ~4 hours  \n\n## ðŸ› The Problem\n\nWhat started as a routine `LangChain` dependency upgrade (`0.3.49` -> `0.3.72`) to fix Docker module resolution issues quickly turned into a critical incident. After upgrading the LangChain packages, the chat functionality completely stopped working across the entire platform. Users could no longer send messages or receive responses from any AI models, effectively rendering the core feature of ChatOllama unusable.\n\nThe issue was particularly frustrating because there were no obvious error messages or warnings during the upgrade process. The application started normally, but every chat attempt simply failed silently.\n\n## ðŸ” Root Cause Investigation\n\nAfter diving into the logs and tracing through the code, we discovered that the LangChain upgrade had introduced breaking API changes in the chat model constructors. What made this especially tricky was that these weren't compile-time errors - the old parameter names were simply ignored, causing the models to initialize with undefined configurations.\n\nDuring the LangChain upgrade process, several parameter names in the ChatOpenAI model constructor underwent changes. While these parameters were merely marked as `deprecated`, their usage in downstream implementations had already changed. The deprecated parameters include:\n\n- `modelName`\n- `openAIApiKey`\n\nThe breaking changes affected multiple model providers, with each requiring specific parameter name updates:\n\n### Before (Working):\n```typescript\nnew ChatOpenAI({\n  configuration: { baseURL },\n  openAIApiKey: params.key,    // âŒ Deprecated\n  modelName: modelName,        // âŒ Deprecated\n})\n\nnew ChatAnthropic({\n  anthropicApiUrl: endpoint,\n  anthropicApiKey: params.key, // âŒ Deprecated  \n  modelName: modelName,        // âŒ Deprecated\n})\n```\n\n### After (Fixed):\n```typescript\nnew ChatOpenAI({\n  configuration: { baseURL },\n  apiKey: params.key,          // âœ… New API\n  model: modelName,            // âœ… New API\n})\n\nnew ChatAnthropic({\n  anthropicApiUrl: endpoint,\n  apiKey: params.key,          // âœ… New API\n  model: modelName,            // âœ… New API\n})\n```\n\n## ðŸ”§ The Fix Implementation\n\nOnce we identified the root cause, the fix was relatively straightforward but required careful attention to detail. We needed to update parameter names across all affected model providers while ensuring backward compatibility and adding better error handling.\n\nThe following models required updates:\n- **OpenAI (ChatOpenAI)** - Most commonly used provider\n- **Anthropic (ChatAnthropic)** - Critical for AI agents functionality \n- **Gemini (ChatGoogleGenerativeAI)** - Used for multimodal features\n- **Groq (ChatGroq)** - High-performance inference option\n\nThe key changes implemented were:\n1. Standardized `openAIApiKey` and `anthropicApiKey` to the unified `apiKey` parameter\n2. Updated `modelName` to the more concise `model` parameter across all providers\n3. Enhanced error handling to provide clear feedback when configurations are missing\n\nBeyond just fixing the parameter names, we took the opportunity to add robust fallback logic. Now, when external API providers fail due to missing keys or configuration issues, the system gracefully falls back to Ollama, ensuring users can continue chatting even if their preferred provider is misconfigured.\n\n## ðŸ“š Lessons Learned\n\nThis incident reinforced several important principles for managing dependencies in production applications:\n\n**Test Thoroughly After Major Upgrades:** Even seemingly minor version bumps can introduce breaking changes that aren't immediately obvious. Comprehensive testing across all features is essential, not just the areas you expect to be affected.\n\n**Embrace API Standardization:** While initially disruptive, LangChain's move to standardize parameter names across providers is a positive long-term change that will reduce confusion and make the codebase more maintainable.\n\n**Always Implement Graceful Degradation:** Having robust fallback mechanisms isn't just good practice - it's essential for maintaining user trust when external dependencies fail or change unexpectedly.\n\n## ðŸš€ Impact and Resolution\n\nThe fix was deployed immediately after identification, resulting in zero downtime for users. The updated implementation maintains full backward compatibility while leveraging the new standardized APIs. As an added benefit, the enhanced error handling and fallback mechanisms have actually improved the overall reliability of the chat system.\n\nThis incident serves as a reminder that in the fast-moving world of AI and machine learning libraries, staying current with dependencies requires constant vigilance and thorough testing practices.\n\n---\n\n*This was a classic case of \"silent\" breaking changes in a major upgrade - the kind that make experienced developers always read changelogs twice. The fix was simple once identified, but the experience highlights why we never take seemingly routine updates for granted.*\n",
      "excerpt": "Date: August 25, 2025  \nIssue: Chat functionality broken after LangChain dependency upgrade  \nResolution Time: ~4 hours  \n\n\n\nWhat started as a routine LangChain dependency upgrade (0.3.49 -> 0.3.72)...",
      "readingTime": 4,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250825-langchain-upgrade-chat-fix.md"
    },
    {
      "slug": "20250819-agents-streaming-implementation",
      "title": "20250819 - ðŸ§ ðŸ¤– DeepAgents Integration: AI Agents Streaming Implementation & UI Enhancement",
      "date": "2025-08-19",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# 20250819 - ðŸ§ ðŸ¤– DeepAgents Integration: AI Agents Streaming Implementation & UI Enhancement\n\n## Overview\n\n**DeepAgents** is a LangChain open sourced AI Agent application development package. It has both Python and JavaScript packages that create \"deep\" agents capable of planning and acting over longer, more complex tasks. The implementation features structured streaming, tool message handling, and internationalization support, with a focus on creating a robust, server-side processed streaming architecture that provides a lightweight client experience.\n\nI integrated DeepAgents into my open source AI chatbot [ChatOllama](https://github.com/sugarforever/chat-ollama). Users now, can chat with AI and work out deep research tasks on ChatOllama. This note recorded my development experience. Hope it helps in your development.\n\n## About DeepAgents\n\n**DeepAgents** represents a significant advancement over simple LLM-tool calling architectures. While using an LLM to call tools in a loop is the simplest form of an agent, this approach often yields \"shallow\" agents that fail to plan and act effectively over complex, multi-step tasks.\n\nDeepAgents solves this limitation by implementing four key components that make agents truly \"deep\":\n\n1. **ðŸŽ¯ Planning Tool**: A built-in planning system that helps agents create and maintain structured plans\n2. **ðŸ¤– Sub Agents**: Specialized agents for specific tasks and context quarantine\n3. **ðŸ“ File System Access**: Mock file system for persistent state and document management\n4. **ðŸ“ Detailed Prompts**: Sophisticated system prompts based on successful applications like Claude Code\n\nThis architecture enables agents to:\n- Break down complex tasks into manageable steps\n- Maintain context across long conversations\n- Delegate specialized work to focused sub-agents\n- Persist and manipulate information through a file system interface\n- Execute sophisticated research and analysis workflows\n\nThe DeepAgents approach has been successfully demonstrated in applications like \"Deep Research\", \"Manus\", and \"Claude Code\", and our implementation brings these capabilities to a general-purpose chat interface.\n\n### How to Use DeepAgents\n\n#### Installation\n\n```bash\nnpm install deepagents @langchain/core langchain-mcp-adapters\n```\n\n#### Basic Usage Example\n\nHere's a simple example of creating a research agent with DeepAgents:\n\n```python\nimport os\nfrom typing import Literal\nfrom tavily import TavilyClient\nfrom deepagents import create_deep_agent\n\n# Initialize search tool\ntavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n\ndef internet_search(\n    query: str,\n    max_results: int = 5,\n    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n    include_raw_content: bool = False,\n):\n    \"\"\"Run a web search\"\"\"\n    return tavily_client.search(\n        query,\n        max_results=max_results,\n        include_raw_content=include_raw_content,\n        topic=topic,\n    )\n\n# Define agent instructions\nresearch_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.\n\nYou have access to a few tools.\n\n## `internet_search`\n\nUse this to run an internet search for a given query. You can specify the number of results, the topic, and whether raw content should be included.\n\"\"\"\n\n# Create the deep agent\nagent = create_deep_agent(\n    [internet_search],\n    research_instructions,\n)\n\n# Invoke the agent\nresult = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is langgraph?\"}]})\n```\n\n#### Custom Sub-Agents\n\nYou can create specialized sub-agents for specific tasks:\n\n```python\n# Define a critique sub-agent\ncritique_sub_agent = {\n    \"name\": \"critique-agent\",\n    \"description\": \"Critique and improve research reports\",\n    \"prompt\": \"You are a tough editor who provides constructive feedback on research reports.\",\n    \"model_settings\": {\n        \"model\": \"anthropic:claude-3-5-haiku-20241022\",\n        \"temperature\": 0,\n        \"max_tokens\": 8192\n    }\n}\n\n# Create agent with sub-agents\nagent = create_deep_agent(\n    tools=[internet_search],\n    instructions=\"You are an expert researcher...\",\n    subagents=[critique_sub_agent],\n)\n```\n\n#### Using Custom Models\n\n```python\nfrom deepagents import create_deep_agent\nfrom langchain.chat_models import init_chat_model\n\n# Use a custom model like Ollama\nmodel = init_chat_model(\n    model=\"ollama:gpt-oss:20b\",  \n)\n\nagent = create_deep_agent(\n    tools=tools,\n    instructions=instructions,\n    model=model,\n)\n```\n\n#### MCP Integration\n\nDeepAgents can work with MCP (Model Context Protocol) tools:\n\n```python\nimport asyncio\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom deepagents import create_deep_agent\n\nasync def main():\n    # Collect MCP tools\n    mcp_client = MultiServerMCPClient(...)\n    mcp_tools = await mcp_client.get_tools()\n\n    # Create agent with MCP tools\n    agent = create_deep_agent(tools=mcp_tools, instructions=\"...\")\n\n    # Stream the agent response\n    async for chunk in agent.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is langgraph?\"}]},\n        stream_mode=\"values\"\n    ):\n        if \"messages\" in chunk:\n            chunk[\"messages\"][-1].pretty_print()\n\nasyncio.run(main())\n```\n\n## ChatOllama DeepAgents Integration\n\n### Server-Side Implementation\n\nOur ChatOllama integration implements DeepAgents on the server side with structured streaming. Here's how we created the agent endpoint:\n\n#### Agent Creation (`server/api/agents/[id].post.ts`)\n\n```typescript\nimport { StructuredTool } from '@langchain/core/tools'\nimport { createDeepAgent } from 'deepagents'\nimport { McpService } from '~/server/utils/mcp'\n\nconst createAgent = (instruction: string, tools: StructuredTool[]) => {\n  const agent = createDeepAgent({\n    tools: tools,\n    instructions: instruction\n  })\n  return agent\n}\n\nexport default defineEventHandler(async (event) => {\n  // Get MCP tools for enhanced capabilities\n  const mcpService = new McpService()\n  const mcpTools = await mcpService.listTools()\n  \n  const { instruction, prompt, conversationRoundId } = await readBody(event)\n  const agent = createAgent(instruction, mcpTools as StructuredTool[])\n\n  // Stream agent responses\n  const responseStream = await agent.stream({\n    \"messages\": [{ \"role\": \"user\", \"content\": prompt }]\n  }, { streamMode: \"values\" })\n\n  // Process and accumulate streaming content\n  const readableStream = Readable.from((async function* () {\n    const aiMessageId = `ai_${conversationRoundId}`\n    let accumulatedAIContent = ''\n    let aiMessageSent = false\n    const toolMessages = new Map()\n    \n    for await (const chunk of responseStream) {\n      const messages = chunk.messages\n      const lastMessage = messages[messages.length - 1]\n      const messageType = lastMessage._getType ? lastMessage._getType() : 'ai'\n      \n      if (messageType === 'ai') {\n        // Process and accumulate AI content\n        let textContent = ''\n        if (Array.isArray(lastMessage.content)) {\n          textContent = lastMessage.content\n            .filter(item => typeof item === 'string' || \n                    (item && typeof item === 'object' && item.type === 'text'))\n            .map(item => typeof item === 'string' ? item : item.text || '')\n            .join(' ')\n        } else {\n          textContent = String(lastMessage.content || '')\n        }\n        \n        if (textContent.length > accumulatedAIContent.length) {\n          accumulatedAIContent = textContent\n          yield JSON.stringify({\n            id: aiMessageId,\n            type: 'ai',\n            content: accumulatedAIContent,\n            conversationRoundId,\n            timestamp: Date.now(),\n            isUpdate: aiMessageSent\n          }) + '\\n'\n          aiMessageSent = true\n        }\n      } else if (messageType === 'tool') {\n        // Process tool messages with deduplication\n        const toolCallId = lastMessage.tool_call_id || `tool_${Date.now()}`\n        if (!toolMessages.has(toolCallId)) {\n          yield JSON.stringify({\n            id: toolCallId,\n            type: 'tool',\n            content: lastMessage.content,\n            name: lastMessage.name || 'Tool',\n            tool_call_id: toolCallId,\n            conversationRoundId,\n            timestamp: Date.now()\n          }) + '\\n'\n          toolMessages.set(toolCallId, true)\n        }\n      }\n    }\n  })())\n\n  return sendStream(event, readableStream)\n})\n```\n\n### Client-Side Implementation\n\n#### Agent Chat Component (`components/AgentChat.vue`)\n\n```vue\n<script setup lang=\"ts\">\nimport { useAgentWorker } from '~/composables/useAgentWorker'\nimport AgentToolMessage from '~/components/AgentToolMessage.vue'\n\nconst { onReceivedMessage, sendMessage } = useAgentWorker()\nconst messages = ref<ChatMessage[]>([])\n\n// Default agent instruction inspired by DeepAgents research example\nconst agentInstruction = ref(`You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.\n\nYou have access to a few tools.\n\nUse this to run an internet search for a given query. You can specify the number of results, the topic, and whether raw content should be included.`)\n\nconst onSend = async (data: ChatBoxFormData) => {\n  const conversationRoundId = crypto.randomUUID()\n  \n  // Add user message\n  const userMessage = createChatMessage({\n    role: \"user\",\n    content: data.content,\n    conversationRoundId\n  })\n  messages.value.push(userMessage)\n\n  // Send to agent with instruction and prompt\n  await sendMessage(conversationRoundId, {\n    instruction: agentInstruction.value,\n    prompt: typeof data.content === 'string' ? data.content : \n            data.content.map(item => item.type === 'text' ? item.text : '').join(' ')\n  })\n}\n\n// Handle streaming responses\nonReceivedMessage((message) => {\n  if (message.type === 'message') {\n    const { messageType, content, conversationRoundId, isUpdate } = message.data\n    \n    if (messageType === 'ai') {\n      // Update or create AI message\n      const existingIndex = messages.value.findIndex(m => \n        m.conversationRoundId === conversationRoundId && m.role === 'assistant')\n      \n      if (existingIndex >= 0 && isUpdate) {\n        messages.value[existingIndex].content = content\n      } else {\n        messages.value.push(createChatMessage({\n          role: \"assistant\",\n          content,\n          conversationRoundId\n        }))\n      }\n    } else if (messageType === 'tool') {\n      // Add tool message\n      messages.value.push(createChatMessage({\n        role: \"tool\",\n        content,\n        contentType: 'tool',\n        toolName: message.data.name,\n        conversationRoundId\n      }))\n    }\n  }\n})\n</script>\n\n<template>\n  <div class=\"agent-chat\">\n    <!-- Render messages with tool message components -->\n    <div v-for=\"message in visibleMessages\" :key=\"message.id\">\n      <AgentToolMessage v-if=\"message.contentType === 'tool'\" \n                        :message=\"message\" />\n      <ChatMessage v-else :message=\"message\" />\n    </div>\n    \n    <!-- Chat input -->\n    <ChatInputBox @send=\"onSend\" />\n  </div>\n</template>\n```\n\n#### Agent Worker for Streaming (`composables/useAgentWorker.ts`)\n\n```typescript\nexport function useAgentWorker() {\n  const handlers: Handler[] = []\n  const abortControllers = new Map<number, AbortController>()\n\n  async function sendAgentRequest(uid: number, conversationRoundId: string, data: AgentRequestData) {\n    const controller = new AbortController()\n    abortControllers.set(uid, controller)\n\n    try {\n      const response = await fetch('/api/agents/1', {\n        method: 'POST',\n        body: JSON.stringify({ ...data, conversationRoundId }),\n        headers: { 'Content-Type': 'application/json' },\n        signal: controller.signal,\n      })\n\n      const reader = response.body?.getReader()\n      const decoder = new TextDecoder()\n\n      while (true) {\n        const { value, done } = await reader!.read()\n        if (done) break\n\n        const chunk = decoder.decode(value)\n        const lines = chunk.split('\\n')\n\n        for (const line of lines) {\n          if (line.trim()) {\n            try {\n              const parsed = JSON.parse(line)\n              handlers.forEach(handler => handler({\n                uid,\n                id: parsed.id,\n                type: 'message',\n                data: {\n                  messageType: parsed.type,\n                  content: parsed.content,\n                  name: parsed.name,\n                  tool_call_id: parsed.tool_call_id,\n                  conversationRoundId: parsed.conversationRoundId,\n                  timestamp: parsed.timestamp,\n                  isUpdate: parsed.isUpdate\n                }\n              }))\n            } catch (e) {\n              console.error('Failed to parse message:', e)\n            }\n          }\n        }\n      }\n    } catch (error) {\n      handlers.forEach(handler => handler({\n        uid, id: '', type: 'error', \n        message: error.message\n      }))\n    }\n  }\n\n  return {\n    onReceivedMessage: (handler: Handler) => handlers.push(handler),\n    sendMessage: async (conversationRoundId: string, data: AgentRequestData) => {\n      const uid = Date.now()\n      await sendAgentRequest(uid, conversationRoundId, data)\n    }\n  }\n}\n```\n\n### Tool Message UI (`components/AgentToolMessage.vue`)\n\n```vue\n<template>\n  <div class=\"tool-message\">\n    <div class=\"tool-header\" @click=\"expanded = !expanded\">\n      <UIcon :name=\"toolIcon\" class=\"tool-icon\" />\n      <span class=\"tool-name\">{{ message.toolName || 'Tool' }}</span>\n      <UIcon :name=\"expanded ? 'i-heroicons-chevron-up' : 'i-heroicons-chevron-down'\" />\n    </div>\n    \n    <div v-if=\"expanded\" class=\"tool-content\">\n      <pre><code v-html=\"highlightedContent\"></code></pre>\n    </div>\n  </div>\n</template>\n\n<script setup lang=\"ts\">\nimport hljs from 'highlight.js'\n\nconst props = defineProps<{ message: ChatMessage }>()\nconst expanded = ref(false)\n\nconst toolIcon = computed(() => {\n  const toolName = props.message.toolName?.toLowerCase() || ''\n  if (toolName.includes('search')) return 'i-heroicons-magnifying-glass'\n  if (toolName.includes('browser')) return 'i-heroicons-globe-alt'\n  if (toolName.includes('file')) return 'i-heroicons-document'\n  if (toolName.includes('calculator')) return 'i-heroicons-calculator'\n  return 'i-heroicons-wrench'\n})\n\nconst highlightedContent = computed(() => {\n  try {\n    const content = typeof props.message.content === 'string' \n      ? props.message.content \n      : JSON.stringify(props.message.content, null, 2)\n    return hljs.highlightAuto(content).value\n  } catch {\n    return props.message.content\n  }\n})\n</script>\n```\n\n## Branch: `feature/deep-agents`\n**Base Branch:** `main`  \n**Development Date:** August 19, 2025\n\n## Key Features Implemented\n\n### 1. AI Agents Chat System\n- **New Agent Chat Component** (`components/AgentChat.vue` - 460 lines)\n  - Dedicated chat interface for AI agents with tool access\n  - Real-time streaming response handling\n  - Conversation round-based UUID grouping\n  - Artifact support integration\n  - Agent instruction management\n\n- **Agent API Endpoint** (`server/api/agents/[id].post.ts` - 127 lines)\n  - Server-side AI content processing and accumulation\n  - Structured streaming with JSON message format\n  - Tool message deduplication and processing\n  - Conversation round tracking with UUIDs\n\n### 2. Structured Streaming Architecture\n\n#### Server-Side Processing\n- **Content Accumulation**: AI responses are accumulated on the server, only sending updates when content grows\n- **Message Type Detection**: Automatic detection of HumanMessage, AIMessage, and ToolMessage types\n- **Content Processing**: Complex array structures are flattened to readable strings\n- **UUID-based Grouping**: Each conversation round gets a unique UUID to group related messages\n\n#### Client-Side Simplification\n- **Lightweight Processing**: Client simply renders server-processed content\n- **Update Flags**: Server indicates whether message is new or an update\n- **No Content Logic**: All accumulation and deduplication handled server-side\n\n### 3. Tool Message UI System\n\n#### AgentToolMessage Component (`components/AgentToolMessage.vue` - 88 lines)\n- **Collapsible Interface**: Tool calls appear as small, expandable elements\n- **Icon Integration**: Automatic icon selection based on tool type (search, browser, file, etc.)\n- **Content Formatting**: Formatted display of tool outputs with syntax highlighting\n- **Expand/Collapse**: Users can view tool details on demand\n\n#### Tool Types Supported\n- Web search tools\n- Browser/navigation tools\n- File operations\n- Calculator functions\n- Code execution\n- Generic tool fallback\n\n### 4. Agent Worker System (`composables/useAgentWorker.ts` - 154 lines)\n- **Streaming Handler**: Manages real-time message streaming from agents API\n- **Message Processing**: Parses JSON-structured messages from server\n- **Error Handling**: Robust error handling for network issues and parsing errors\n- **Abort Functionality**: Ability to cancel ongoing agent requests\n\n### 5. Internationalization (i18n) Support\n\n#### English Locale (`locales/en-US.json`)\n```json\n\"agents\": {\n  \"title\": \"AI Agent Chat\",\n  \"welcome\": \"Welcome to AI Agents\",\n  \"welcomeMessage\": \"Start chatting with your AI agent. It has access to various tools to help you accomplish tasks.\",\n  \"inputPlaceholder\": \"Ask the agent to help you with anything...\"\n}\n```\n\n#### Chinese Locale (`locales/zh-CN.json`)\n```json\n\"agents\": {\n  \"title\": \"AI æ™ºèƒ½ä½“èŠå¤©\",\n  \"welcome\": \"æ¬¢è¿Žä½¿ç”¨ AI æ™ºèƒ½ä½“\",\n  \"welcomeMessage\": \"å¼€å§‹ä¸Žæ‚¨çš„ AI æ™ºèƒ½ä½“èŠå¤©ã€‚å®ƒå¯ä»¥ä½¿ç”¨å„ç§å·¥å…·æ¥å¸®åŠ©æ‚¨å®Œæˆä»»åŠ¡ã€‚\",\n  \"inputPlaceholder\": \"è¯·å‘Šè¯‰æ™ºèƒ½ä½“æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©...\"\n}\n```\n\n### 6. Type System Enhancements (`types/chat.d.ts`)\n- **Extended ChatMessage Interface**: Added support for tool messages and agent-specific properties\n- **Content Type Extensions**: Support for `'tool'` content type\n- **Message Type Extensions**: Added `'tool'` message type\n- **Agent Properties**: `messageType`, `toolName`, `additionalKwargs` fields\n\n## Technical Architecture\n\n### Message Flow\n1. **User Input** â†’ Generate conversation round UUID\n2. **Client Request** â†’ Send to agents API with UUID\n3. **Server Processing** â†’ DeepAgents streaming with content accumulation\n4. **Structured Output** â†’ JSON messages with type, content, and metadata\n5. **Client Rendering** â†’ Simple message display and tool UI components\n\n### Streaming Protocol\n```json\n// AI Message\n{\n  \"id\": \"ai_uuid-generated-id\",\n  \"type\": \"ai\",\n  \"content\": \"Accumulated AI response text...\",\n  \"conversationRoundId\": \"conversation-uuid\",\n  \"timestamp\": 1692455200000,\n  \"isUpdate\": true\n}\n\n// Tool Message\n{\n  \"id\": \"tool_unique-id\",\n  \"type\": \"tool\",\n  \"content\": \"Formatted tool output...\",\n  \"name\": \"search\",\n  \"tool_call_id\": \"call_xyz\",\n  \"conversationRoundId\": \"conversation-uuid\",\n  \"timestamp\": 1692455200000\n}\n```\n\n### Agent Configuration\n- **Default Instruction**: Expert researcher with access to tools for thorough research and report writing (inspired by DeepAgents research agent example)\n- **DeepAgents Integration**: Full integration with DeepAgents' planning tools, sub-agents, and file system\n- **Tool Integration**: MCP (Model Context Protocol) servers for enhanced capabilities\n- **Dynamic Instructions**: Users can modify agent instructions in real-time\n- **Built-in Capabilities**: \n  - Planning tool for task breakdown and tracking\n  - General-purpose sub-agent for specialized tasks\n  - Mock file system for document persistence\n  - Context quarantine through sub-agent delegation\n\n## Key Technical Decisions\n\n### 1. Server-Side Processing Priority\n**Decision**: Move all message processing, accumulation, and deduplication to the server  \n**Rationale**: Provides consistent behavior, reduces client complexity, and ensures reliable streaming\n\n### 2. UUID-Based Conversation Rounds\n**Decision**: Generate unique UUIDs for each user question/agent response cycle  \n**Rationale**: Clean separation between conversation rounds, enables future features like editing specific rounds\n\n### 3. Separate Tool Message UI\n**Decision**: Render tool calls as separate, collapsible UI elements rather than inline text  \n**Rationale**: Better user experience, clear visibility of agent actions, optional detail viewing\n\n### 4. JSON Streaming Protocol\n**Decision**: Use newline-delimited JSON for streaming instead of raw text  \n**Rationale**: Structured data enables rich message types, metadata, and client-side rendering logic\n\n## Dependencies Added\n- **deepagents**: Core \"deep\" AI agent functionality with planning, sub-agents, and file system capabilities\n- **@langchain/core**: Tool integration and structured AI workflows\n- **langchain-mcp-adapters**: MCP (Model Context Protocol) integration for enhanced tool access\n\n## File Changes Summary\n- **New Files**: 4 (AgentChat.vue, AgentToolMessage.vue, useAgentWorker.ts, agents/[id].post.ts, agents/index.vue)\n- **Modified Files**: 4 (en-US.json, zh-CN.json, chat.d.ts, package.json)\n- **Total Lines Added**: ~1,079 lines\n- **Total Lines Removed**: ~44 lines\n\n## Issues Resolved\n\n### 1. AI Message Accumulation Problem\n**Issue**: AI messages were being replaced instead of accumulated during streaming  \n**Solution**: Server-side content accumulation with length-based update detection\n\n### 2. Tool Message Visibility\n**Issue**: Tool calls were not clearly visible to users  \n**Solution**: Dedicated collapsible UI components with tool-specific icons\n\n### 3. Conversation Round Separation\n**Issue**: Multiple conversation rounds were getting mixed together  \n**Solution**: UUID-based grouping system for each user question/agent response cycle\n\n### 4. Streaming Chunk Type Errors\n**Issue**: Backend streaming was sending array content that caused chunk type errors  \n**Solution**: Server-side content processing to ensure string output for streaming\n\n## Future Enhancements\n\n### 1. Agent Marketplace\n- Support for multiple pre-configured agent types\n- Agent template system with specialized instructions\n- Community sharing of agent configurations\n\n### 2. Enhanced Tool Integration\n- Visual tool execution indicators\n- Tool output formatting improvements\n- Real-time tool execution status\n\n### 3. Conversation Management\n- Edit and regenerate specific conversation rounds\n- Export conversation transcripts\n- Conversation search and filtering\n\n### 4. Performance Optimizations\n- Message virtualization for long conversations\n- Background processing for large tool outputs\n- Caching for frequently used tools\n\n## Testing Recommendations\n1. **Streaming Reliability**: Test with slow networks and interruptions\n2. **Tool Message Rendering**: Verify all tool types display correctly\n3. **UUID Collision**: Test with rapid successive requests\n4. **Internationalization**: Verify text rendering in all supported languages\n5. **Memory Usage**: Test with very long conversations\n\n## Deployment Notes\n- **Environment Variables**: Ensure MCP server configuration is properly set\n- **Tool Dependencies**: Verify all required tools and services are available\n- **Streaming Support**: Ensure deployment platform supports Server-Sent Events\n- **CORS Configuration**: May need updates for tool-related external requests\n\n---\n\n**Contributors**: Claude Code Assistant  \n**Review Status**: Ready for review  \n**Merge Target**: `main` branch",
      "excerpt": "DeepAgents is a LangChain open sourced AI Agent application development package. It has both Python and JavaScript packages that create \"deep\" agents capable of planning and acting over longer, more...",
      "readingTime": 14,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250819-agents-streaming-implementation.md"
    },
    {
      "slug": "20250819-feature-flags-in-docker-and-nuxt",
      "title": "Feature Flags in Docker: Why MCP_ENABLED Didnâ€™t Work and How We Fixed It",
      "date": "2025-08-19",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# Feature Flags in Docker: Why MCP_ENABLED Didnâ€™t Work and How We Fixed It\n\n*August 19, 2025*\n\nHey everyone! ðŸ‘‹\n\nFollowing yesterdayâ€™s UI improvements post, I dug into a deployment gotcha that bit us when running in Docker: feature flags like MCP worked locally but not inside containers. Hereâ€™s what happened and how to fix it.\n\n## ðŸ› The Symptom\n\n- **Local dev**: Setting `MCP_ENABLED=true` in `.env` made the Settings â†’ MCP module appear.\n- **Docker**: Setting `MCP_ENABLED=true` in `docker-compose.yaml` did nothing â€” the MCP section didnâ€™t show up.\n\n## ðŸ”Ž Root Cause: Nuxt runtimeConfig (build-time vs runtime)\n\nNuxt 3 reads `runtimeConfig` values at build time via `process.env`. At runtime, overriding them requires environment variables that map to config keys with the `NUXT_` prefix.\n\nOur `nuxt.config.ts` had:\n\n```ts\nruntimeConfig: {\n  knowledgeBaseEnabled: process.env.KNOWLEDGE_BASE_ENABLED === 'true',\n  realtimeChatEnabled: process.env.REALTIME_CHAT_ENABLED === 'true',\n  modelsManagementEnabled: process.env.MODELS_MANAGEMENT_ENABLED === 'true',\n  mcpEnabled: process.env.MCP_ENABLED === 'true',\n  public: { /* ... */ }\n}\n```\n\n- In dev, `.env` is loaded before build, so `process.env.MCP_ENABLED` was true when we built â†’ `mcpEnabled` baked as true.\n- In Docker, we used a prebuilt image. Setting `MCP_ENABLED=true` at runtime does not change `runtimeConfig.mcpEnabled`. You must use `NUXT_MCP_ENABLED=true` to override at runtime.\n\nThis explains why `/api/features` logs showed `process.env.MCP_ENABLED` as true, but `useRuntimeConfig().mcpEnabled` stayed false.\n\n## âœ… The Fix\n\n### Option A (Recommended): Use `NUXT_`-prefixed env vars in Docker\n\nUpdate `docker-compose.yaml`:\n\n```yaml\nservices:\n  chatollama:\n    environment:\n      - NUXT_MCP_ENABLED=true\n      - NUXT_KNOWLEDGE_BASE_ENABLED=true\n      - NUXT_REALTIME_CHAT_ENABLED=true\n      - NUXT_MODELS_MANAGEMENT_ENABLED=true\n```\n\nThis maps directly to `runtimeConfig` at runtime â€” no code changes needed.\n\n### Option B: Support both legacy and `NUXT_` in code\n\nIf you want `MCP_ENABLED` to keep working, make `nuxt.config.ts` prefer the runtime `NUXT_` variables and fall back to the legacy ones:\n\n```ts\nruntimeConfig: {\n  knowledgeBaseEnabled: process.env.NUXT_KNOWLEDGE_BASE_ENABLED === 'true' || process.env.KNOWLEDGE_BASE_ENABLED === 'true',\n  realtimeChatEnabled: process.env.NUXT_REALTIME_CHAT_ENABLED === 'true' || process.env.REALTIME_CHAT_ENABLED === 'true',\n  modelsManagementEnabled: process.env.NUXT_MODELS_MANAGEMENT_ENABLED === 'true' || process.env.MODELS_MANAGEMENT_ENABLED === 'true',\n  mcpEnabled: process.env.NUXT_MCP_ENABLED === 'true' || process.env.MCP_ENABLED === 'true',\n  public: { /* ... */ }\n}\n```\n\n## ðŸ”§ How to Verify\n\n1. Redeploy with the updated Compose env vars.\n2. Hit `/api/features` and check logs â€” they print both environment vars and `runtimeConfig` values.\n3. Open Settings: the MCP section should appear when `mcpEnabled` is true.\n\n## ðŸ¤” Why it worked locally but not in Docker\n\n- **Local**: `.env` loaded before build â†’ `runtimeConfig` baked with your values.\n- **Docker**: prebuilt image â†’ runtime overrides require `NUXT_`-prefixed variables.\n\n## ðŸ“ Small DX touch-up (optional)\n\n- Add `modelsManagementEnabled` to the `FeatureFlags` interface in `composables/useFeatures.ts` for type completeness.\n\n## ðŸŽ¯ Takeaway\n\nRemember this rule of thumb with Nuxt 3: build-time envs bake defaults; runtime overrides need `NUXT_`. With that in place, the Settings page correctly reflects features across environments.\n",
      "excerpt": "August 19, 2025\n\nHey everyone! ðŸ‘‹\n\nFollowing yesterdayâ€™s UI improvements post, I dug into a deployment gotcha that bit us when running in Docker: feature flags like MCP worked locally but not inside...",
      "readingTime": 3,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250819-feature-flags-in-docker-and-nuxt.md"
    },
    {
      "slug": "20250818-ui-improvements-and-chat-fixes",
      "title": "UI Improvements and Chat Reliability Fixes",
      "date": "2025-08-18",
      "author": "ChatOllama Team",
      "tags": [],
      "language": "en",
      "content": "# UI Improvements and Chat Reliability Fixes\n\n*August 18, 2025*\n\nHey everyone! ðŸ‘‹\n\nI've been working on some important improvements to the chat interface over the past few days. Here's what's new and what got fixed.\n\n## ðŸ› Major Bug Fixes\n\n### Chat Creation Button Issues\nOne of the most frustrating bugs was the unresponsive \"create new chat\" button. Users would click it, nothing would happen, then they'd click multiple times and suddenly get several new chats created at once. \n\n**What was happening:**\n- The `scrollToBottom` function was trying to access `messageListEl.value.scrollHeight` before the DOM element was ready\n- No loading state protection meant rapid clicks could trigger multiple API calls\n- Race conditions in the chat creation flow\n\n**The fix:**\n```javascript\n// Added null check in scrollToBottom\nconst scrollToBottom = (_behavior: ScrollBehavior) => {\n    behavior.value = _behavior\n    if (messageListEl.value) {\n        y.value = messageListEl.value.scrollHeight\n    }\n}\n\n// Added loading state in ChatSessionList\nconst isCreatingChat = ref(false)\n\nasync function onNewChat() {\n    if (isCreatingChat.value) return\n    \n    isCreatingChat.value = true\n    try {\n        const data = await createChatSession()\n        sessionList.value.unshift(data)\n        await router.push(`/chat/${data.id}`)\n    } finally {\n        isCreatingChat.value = false\n    }\n}\n```\n\nThis was a classic example of how small timing issues can create really annoying UX problems!\n\n## âœ¨ New Feature: Enhanced Preview Panel\n\nThe artifact preview system got a major upgrade! Previously, users could only view code artifacts in a basic side panel. Now we have:\n\n### Split View Mode\n- Chat takes up the remaining space\n- Preview panel has a fixed 500px width\n- Both are visible simultaneously for context\n\n### Fullscreen Mode\n- Preview covers the entire viewport\n- Header is completely hidden for maximum viewing area\n- Floating close button with semi-transparent background\n- Perfect for viewing complex HTML demos or detailed diagrams\n\n### Smart State Management\nThis was trickier than it sounds. The key insight was separating the \"show/hide preview\" state from the \"normal/fullscreen\" state:\n\n```javascript\n// Two separate states instead of one confusing state\nconst showArtifacts = ref(false)\nconst isFullscreen = ref(false)\n\n// Smart close behavior\nconst closeArtifacts = () => {\n    showArtifacts.value = false\n    isFullscreen.value = false  // Reset fullscreen when closing\n}\n\n// Fullscreen close just exits fullscreen, doesn't close preview\nconst toggleFullscreen = () => {\n    isFullscreen.value = !isFullscreen.value\n}\n```\n\nThe UX flow is now:\n1. Click preview â†’ Opens in split view\n2. Click fullscreen â†’ Expands to fullscreen\n3. Click X in fullscreen â†’ Returns to split view\n4. Click X in split view â†’ Closes preview completely\n\n## ðŸŽ¨ Animation Polish\n\nChanged the preview icon animation from a slide-in effect to a fade-in effect. Sometimes the smallest changes make the biggest difference in how polished an interface feels.\n\n```scss\n// Before: Slide in from right\n.artifact-btn {\n    opacity: 0;\n    transform: translateX(8px);\n}\n\n// After: Simple fade\n.artifact-btn {\n    opacity: 0;\n    transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);\n}\n```\n\n## ðŸ“š What I Learned\n\n### 1. DOM Timing Issues Are Everywhere\nThe `scrollToBottom` bug was a reminder that Vue's reactivity is fast, but the DOM still needs time to update. Always check if elements exist before accessing their properties.\n\n### 2. State Management Complexity\nInitially, I tried to make the preview system \"smart\" with resizable splits and complex state. But simpler is often better - two clear modes (split/fullscreen) with obvious transitions work much better for users.\n\n### 3. User Testing Reveals Edge Cases\nThe chat creation bug only happened under specific timing conditions. Real user behavior (rapid clicking when something seems broken) often reveals issues that don't show up in normal development testing.\n\n## ðŸ’­ Thoughts for Fellow Developers\n\nThese kinds of UI reliability fixes might not be glamorous, but they have huge impact on user experience. A button that works 95% of the time feels broken to users. Taking the time to handle edge cases and race conditions is what separates good interfaces from great ones.\n\nAlso, when building preview/modal systems, always think about the exit flow as much as the entry flow. Users need to understand how to get back to where they came from!\n\n---\n\n*What features would you like to see improved next? Drop your thoughts in the issues!*\n\n*- Your dev team*",
      "excerpt": "August 18, 2025\n\nHey everyone! ðŸ‘‹\n\nI've been working on some important improvements to the chat interface over the past few days. Here's what's new and what got fixed.\n\n\n\n\nOne of the most frustrating...",
      "readingTime": 4,
      "path": "/Users/wyang14/github/chat-ollama/blogs/20250818-ui-improvements-and-chat-fixes.md"
    }
  ],
  "total": 9,
  "generated": "2025-09-15T00:44:05.214Z"
}